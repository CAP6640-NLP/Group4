{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Script Assignment 2\n",
    "### Feb 15, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading our corpus data, which is a list of tech news articles found from kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/jeet2016/us-financial-news-articles\n",
    "\n",
    "There is a requirements.txt file with all the needed python modules, \"pip install -r requirements.txt\"\n",
    "\n",
    "Old: Prior to running, download corpus from top right of link and then extract to a folder called \"archive\" in the same directory as this file\n",
    "New: Extract archive.zip (it's 1.89GB) to ./archive_usfinancial\n",
    "\n",
    "\n",
    "------------- ./ScriptAssignment2.ipynb\n",
    "\n",
    "------------- ./archive_usfinancial/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "nltk.download('punkt')  # one time execution if not cached yet\n",
    "nltk.download('stopwords')  # one time execution if not cached yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection & Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(archives_dir):\n",
    "    corpus = []\n",
    "    counter = 0\n",
    "    # os.walk gets all the folders and paths in given directory\n",
    "    for root, dirs, files in os.walk(archives_dir):\n",
    "        for i, file in enumerate(files):\n",
    "            # join path(root) and filename so that this can be computer-independent as long as names are the same\n",
    "            # open in readonly mode, need encoding and ignore errors for weird characters in docs\n",
    "            with open(os.path.join(root, file), 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "                # load json, can use strings for index from json tree\n",
    "                # if data you want is inside of some structure, can use index like ['test1']['title']\n",
    "                # https://devqa.io/python-parse-json/\n",
    "                \n",
    "                data = json.load(f)\n",
    "                appendme = data['title'] + \" \" + data['published'] + \" \" + data['text'] # Create string with title, publish date, and document text\n",
    "                \n",
    "                if appendme: # Make sure not empty string\n",
    "                    corpus.append((appendme))\n",
    "            \n",
    "            # This is here to only go through a few documents instead of the 2GB of documents\n",
    "            # Leave in place to speed up testing/debug\n",
    "            # Uncomment for real data/tests\n",
    "            counter += 1\n",
    "            if counter > 99: # Break because a lot of documents\n",
    "                return corpus\n",
    "            \n",
    "# Extracted the articles into folder called \"archive_usfinancial\", change as needed\n",
    "# read_data() will parse through all folders/files inside there\n",
    "\n",
    "######################################################\n",
    "########### WARNING: There are a lot of files in there and it took an hour and a half to extract on my desktop\n",
    "######################################################\n",
    "corpus = read_data(os.getcwd() + \"\\\\archive_usfinancial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'emerging',\n",
       " 'market',\n",
       " 'etf',\n",
       " 'eem',\n",
       " 'was',\n",
       " 'a',\n",
       " 'weak',\n",
       " 'dollar',\n",
       " 'and']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    regex = re.compile('[^a-zA-Z0-9]')  # regex to only match alphanumeric characters\n",
    "    tokens = []\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(corpus):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            clean_word = regex.sub('', word)\n",
    "            if clean_word:  # make sure we're not appending empty strings\n",
    "                tokens.append(clean_word.lower())\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(corpus[0])\n",
    "tokens[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charts',\n",
       " 'dollartracking',\n",
       " 'etf',\n",
       " 'uup',\n",
       " 'believes',\n",
       " 'us',\n",
       " 'currency',\n",
       " 'continue',\n",
       " 'help',\n",
       " 'group']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "no_stopwords = remove_stopwords(tokens)\n",
    "no_stopwords[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chart',\n",
       " 'dollartrack',\n",
       " 'etf',\n",
       " 'uup',\n",
       " 'believ',\n",
       " 'us',\n",
       " 'currenc',\n",
       " 'continu',\n",
       " 'help',\n",
       " 'group']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(word):\n",
    "    return stemmer.stem(word).strip()\n",
    "\n",
    "stemmed = [stem_words(word) for word in no_stopwords]\n",
    "stemmed[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Re-do the tfid vectorizer with the preprocessing as a single function\n",
    "# @TODO: Maybe it's not needed though\n",
    "# @TODO: same todo as below\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# def preprocess_corpus(corpus):\n",
    "#     # Tokenize\n",
    "#     regex = re.compile('[^a-zA-Z0-9]')  # regex to only match alphanumeric characters\n",
    "#     tokens = []\n",
    "    \n",
    "#     for sent in nltk.sent_tokenize(corpus):\n",
    "#         for word in nltk.word_tokenize(sent):\n",
    "#             clean_word = regex.sub('', word)\n",
    "#             if clean_word:  # make sure we're not appending empty strings\n",
    "#                 tokens.append(clean_word.lower())\n",
    "\n",
    "#     print(tokens[50:60]) # Print for debug\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stopwords = nltk.corpus.stopwords.words('english')\n",
    "#     no_stopwords = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "#     print(no_stopwords[50:60]) # Print for debug\n",
    "\n",
    "#     # Stemming\n",
    "#     stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#     stemmed = [stemmer.stem(word).strip()for word in no_stopwords]\n",
    "#     print(stemmed[50:60]) # Print for debug\n",
    "        \n",
    "#     return stemmed\n",
    "     \n",
    "# processed_data = preprocess_corpus(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, max_features=50000,\n",
    "                        min_df=0.0, stop_words='english',\n",
    "                        use_idf=True, tokenizer=tokenize,\n",
    "                        preprocessor=stem_words, lowercase=True)\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Re-do the tfid vectorizer with the preprocessing as a single function\n",
    "# @TODO: Maybe it's not needed though\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf2 = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "#                         min_df=0.05, stop_words='english',\n",
    "#                         use_idf=True,\n",
    "#                         preprocessor=preprocess_corpus, lowercase=True)\n",
    "# tfidf_matrix2 = tfidf2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "['auto', 'johnson', 'bitcoin', 'china', 'wechat', 'tax', 'products', 'macy', 'euro', 's']\n",
      "\n",
      "\n",
      "Topic #1:\n",
      "['new', '2017', 'exits', 'korea', 'said', 'bitcoin', 'percent', 'year', 'billion', 's']\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "['ripple', 'tax', 'said', 'ago', '0054', 'market', 'cramer', 's', '2018', 'percent']\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "['netflix', 'said', 'company', 'hatch', 'peso', '1', 'caterpillar', 'cousins', 'compass', 's']\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "['sales', 'intel', 'vehicles', 'apple', 'google', 'cramer', 'china', 'amazon', 'said', 's']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @TODO: Need to reformat and comment/write this better for readability and understanding\n",
    "# @TODO: Also need to figure out what kind of results/metrics we can get out of this and report\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    print(f'Topic #{i}:')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02273976 0.02315068 0.02306682 0.02297623 0.90806652]\n",
      " [0.03833296 0.03854979 0.84612433 0.03850144 0.03849147]\n",
      " [0.02253547 0.02278043 0.02255792 0.02258548 0.9095407 ]\n",
      " [0.01975504 0.01985698 0.01981988 0.92079696 0.01977115]\n",
      " [0.02753668 0.02772577 0.02758118 0.88962008 0.02753629]\n",
      " [0.01875312 0.9248519  0.01882688 0.01878142 0.01878669]\n",
      " [0.0169076  0.0170438  0.01696315 0.01698488 0.93210057]\n",
      " [0.01790418 0.92802594 0.01793719 0.01794733 0.01818536]\n",
      " [0.01892415 0.01919064 0.92377184 0.01903496 0.01907841]\n",
      " [0.01956033 0.01966698 0.01959953 0.01956491 0.92160825]\n",
      " [0.020972   0.91587717 0.02106652 0.0210008  0.02108351]\n",
      " [0.01889165 0.01894183 0.01884567 0.92450361 0.01881723]\n",
      " [0.02533406 0.02545877 0.89839101 0.0253423  0.02547385]\n",
      " [0.02002136 0.02006516 0.02000922 0.02000469 0.91989957]\n",
      " [0.01885969 0.01901186 0.01885303 0.92439534 0.01888008]\n",
      " [0.01291434 0.01307098 0.94818104 0.01291198 0.01292166]\n",
      " [0.04783596 0.808299   0.04808998 0.04771322 0.04806184]\n",
      " [0.01504226 0.01510288 0.01505367 0.93979167 0.01500952]\n",
      " [0.91836581 0.02051468 0.02032624 0.0203464  0.02044686]\n",
      " [0.02160425 0.02178458 0.02164818 0.02173158 0.91323141]\n",
      " [0.01918124 0.92306446 0.01921875 0.01930559 0.01922997]\n",
      " [0.02061754 0.0208063  0.02065581 0.02066107 0.91725927]\n",
      " [0.01515295 0.9391317  0.01519698 0.01526692 0.01525145]\n",
      " [0.0326786  0.0328433  0.86888466 0.03273758 0.03285586]\n",
      " [0.02550737 0.02574976 0.89765167 0.02552031 0.0255709 ]\n",
      " [0.05211816 0.05268668 0.79066678 0.05219023 0.05233815]\n",
      " [0.03344226 0.86547244 0.03375709 0.03375059 0.03357762]\n",
      " [0.0108666  0.01104192 0.01095026 0.95596905 0.01117216]\n",
      " [0.91708125 0.02076097 0.02065636 0.02078801 0.02071341]\n",
      " [0.04141151 0.04131966 0.8345009  0.04137078 0.04139715]\n",
      " [0.01975905 0.0201471  0.01998013 0.01989268 0.92022105]\n",
      " [0.01563685 0.93722866 0.01565809 0.01576805 0.01570835]\n",
      " [0.05608838 0.0561658  0.77541537 0.05610062 0.05622983]\n",
      " [0.93477937 0.01665641 0.01620364 0.01616475 0.01619583]\n",
      " [0.01664274 0.93319966 0.01666816 0.01655219 0.01693725]\n",
      " [0.80533457 0.04835865 0.04885848 0.04887505 0.04857326]\n",
      " [0.01911669 0.01945624 0.01922369 0.92294524 0.01925815]\n",
      " [0.94698476 0.01345415 0.01323145 0.01315088 0.01317876]\n",
      " [0.01955291 0.92175931 0.01954611 0.01960868 0.01953299]\n",
      " [0.04119162 0.83494404 0.0414266  0.04121024 0.0412275 ]\n",
      " [0.02452264 0.02491118 0.90118623 0.024761   0.02461894]\n",
      " [0.01765067 0.92978182 0.01750723 0.0175081  0.01755217]\n",
      " [0.03735742 0.0377208  0.85000678 0.03738701 0.03752799]\n",
      " [0.03206749 0.03233393 0.03211273 0.03217544 0.87131041]\n",
      " [0.02084991 0.91644717 0.02088584 0.02086323 0.02095385]\n",
      " [0.01780843 0.01800796 0.01780212 0.01788022 0.92850127]\n",
      " [0.01743459 0.92997402 0.01757269 0.01748907 0.01752963]\n",
      " [0.87489905 0.03142003 0.03112331 0.03131476 0.03124286]\n",
      " [0.9388182  0.01543593 0.01523446 0.01525771 0.0152537 ]\n",
      " [0.0172828  0.01731014 0.01731078 0.01728086 0.93081542]\n",
      " [0.02755544 0.02764278 0.0279278  0.88879561 0.02807836]\n",
      " [0.01841403 0.92599953 0.01862761 0.01849414 0.01846469]\n",
      " [0.01555934 0.01561044 0.01551481 0.93775585 0.01555956]\n",
      " [0.01451864 0.01465962 0.01451588 0.94176269 0.01454316]\n",
      " [0.01207239 0.01225318 0.01211463 0.95142557 0.01213424]\n",
      " [0.77586771 0.05608909 0.05652523 0.05559027 0.0559277 ]\n",
      " [0.02233775 0.02248674 0.9103688  0.02225992 0.02254679]\n",
      " [0.02254615 0.90959039 0.02256572 0.02270023 0.02259751]\n",
      " [0.01643845 0.01649404 0.01647339 0.01645278 0.93414134]\n",
      " [0.03836347 0.03857868 0.03860938 0.03845862 0.84598985]\n",
      " [0.03791698 0.03877188 0.847289   0.03805165 0.0379705 ]\n",
      " [0.02578504 0.02610504 0.02593933 0.89618958 0.02598101]\n",
      " [0.04858677 0.04882374 0.04868753 0.04860803 0.80529393]\n",
      " [0.01680715 0.01699808 0.01691021 0.01697257 0.93231199]\n",
      " [0.01805115 0.01818519 0.92758391 0.01812794 0.01805181]\n",
      " [0.01690444 0.93204956 0.01697246 0.01714695 0.01692658]\n",
      " [0.01531732 0.01551922 0.01535588 0.93848149 0.01532608]\n",
      " [0.01657234 0.93336816 0.01679507 0.01657907 0.01668536]\n",
      " [0.04694131 0.04688035 0.04712911 0.04676463 0.8122846 ]\n",
      " [0.01484899 0.94035426 0.01491983 0.01491289 0.01496404]\n",
      " [0.01572387 0.9369491  0.01573867 0.0158489  0.01573946]\n",
      " [0.92445137 0.01920861 0.01869453 0.0186601  0.0189854 ]\n",
      " [0.01601692 0.01622497 0.01601195 0.01608184 0.9356643 ]\n",
      " [0.02655016 0.89370371 0.02667476 0.02652372 0.02654765]\n",
      " [0.02199733 0.02221155 0.91093404 0.02216337 0.0226937 ]\n",
      " [0.01322887 0.94685436 0.01322869 0.01330027 0.01338782]\n",
      " [0.03997075 0.04025632 0.04017951 0.83942103 0.04017238]\n",
      " [0.92064432 0.02007582 0.01975798 0.01980982 0.01971206]\n",
      " [0.02000812 0.02057582 0.91910071 0.02007866 0.02023669]\n",
      " [0.02276385 0.02320205 0.02291614 0.02286258 0.90825538]\n",
      " [0.90690923 0.02354933 0.02323536 0.02312372 0.02318236]\n",
      " [0.04083863 0.04114851 0.04109479 0.83581427 0.04110381]\n",
      " [0.02182853 0.91235603 0.0219398  0.02197917 0.02189647]\n",
      " [0.02226505 0.0226281  0.91031044 0.02237041 0.022426  ]\n",
      " [0.03669445 0.03723244 0.03685689 0.8523265  0.03688972]\n",
      " [0.03986973 0.0401576  0.84005677 0.03986986 0.04004603]\n",
      " [0.93607144 0.01610337 0.01593292 0.01594923 0.01594303]\n",
      " [0.01737672 0.01777463 0.01755324 0.92965898 0.01763643]\n",
      " [0.02447209 0.02539114 0.02452014 0.90105953 0.0245571 ]\n",
      " [0.01898944 0.019236   0.92348759 0.01909614 0.01919084]\n",
      " [0.01855059 0.01897051 0.0186483  0.01866051 0.92517009]\n",
      " [0.01464989 0.01479615 0.01472606 0.94111045 0.01471746]\n",
      " [0.02105613 0.91560953 0.02109334 0.02107623 0.02116477]\n",
      " [0.03460074 0.03483083 0.03463161 0.03453469 0.86140214]\n",
      " [0.015335   0.93862222 0.01535643 0.01532438 0.01536198]\n",
      " [0.04456278 0.8216953  0.04456287 0.04451153 0.04466751]\n",
      " [0.92698206 0.01823568 0.0182013  0.01810529 0.01847567]\n",
      " [0.02063926 0.02086537 0.02071257 0.02063795 0.91714484]\n",
      " [0.01662804 0.01709739 0.93266112 0.01674135 0.01687211]\n",
      " [0.01560324 0.01568795 0.93737372 0.01564841 0.01568668]]\n",
      "[4 2 4 3 3 1 4 1 2 4 1 3 2 4 3 2 1 3 0 4 1 4 1 2 2 2 1 3 0 2 4 1 2 0 1 0 3\n",
      " 0 1 1 2 1 2 4 1 4 1 0 0 4 3 1 3 3 3 0 2 1 4 4 2 3 4 4 2 1 3 1 4 1 1 0 4 1\n",
      " 2 1 3 0 2 4 0 3 1 2 3 2 0 3 3 2 4 3 1 4 1 1 0 4 2 2]\n",
      "Document number:  100\n",
      "Topic number:  5\n",
      "100\n",
      "Topic Counts:  Counter({1: 26, 4: 21, 2: 21, 3: 19, 0: 13})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic number: \u001b[39m\u001b[38;5;124m\"\u001b[39m, topic_num)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(topic_values))\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtopic_bar_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_topics\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 12\u001b[0m, in \u001b[0;36mtopic_bar_plot\u001b[1;34m(document_topics)\u001b[0m\n\u001b[0;32m      9\u001b[0m topics \u001b[38;5;241m=\u001b[39m topic_counts\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     10\u001b[0m num_documents \u001b[38;5;241m=\u001b[39m topic_counts\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m(topics, num_documents)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic Number\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Documents\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\matplotlib\\_api\\__init__.py:226\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'bar'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Returns a bar plot showing the distribution of documents among topics\n",
    "'''\n",
    "def topic_bar_plot(document_topics):\n",
    "    # Obtain a dictionary that tells us in how many documents a topic was found in\n",
    "    topic_counts = Counter(document_topics)\n",
    "    print(\"Topic Counts: \", topic_counts)\n",
    "\n",
    "    topics = topic_counts.keys()\n",
    "    num_documents = topic_counts.values()\n",
    "\n",
    "    plt.bar(topics, num_documents)\n",
    "    plt.xlabel('Topic Number')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.title('Number of Documents per Topic')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the distribution of articles among the topics \n",
    "topic_values = lda.transform(tfidf_matrix)\n",
    "print(topic_values)\n",
    "\n",
    "# Get the most well-represented topic for each document\n",
    "document_topics = topic_values.argmax(axis=1)\n",
    "print(document_topics)\n",
    "\n",
    "doc_num, topic_num = topic_values.shape\n",
    "\n",
    "print(\"Document number: \", doc_num)\n",
    "print(\"Topic number: \", topic_num)\n",
    "print(len(topic_values))\n",
    "\n",
    "topic_bar_plot(document_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
