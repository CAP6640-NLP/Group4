{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAP 6640 \n",
    "# Script Assignment 1\n",
    "# Jan 25, 2024\n",
    "\n",
    "# Group 4\n",
    "# Andres Graterol\n",
    "#                   UCF ID: 4031393\n",
    "# Zachary Lyons\n",
    "#                   UCF ID: 4226832\n",
    "# Christopher Hinkle\n",
    "#                   UCF ID: 4038573\n",
    "# Nicolas Leocadio\n",
    "#                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading our corpus data, which is a list of tech news articles found from kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/jeet2016/us-financial-news-articles\n",
    "\n",
    "There is a requirements.txt file with all the needed python modules, \"pip install -r requirements.txt\"\n",
    "\n",
    "Old: Prior to running, download corpus from top right of link and then extract to a folder called \"archive\" in the same directory as this file\n",
    "New: Extract archive.zip (its 1.5GB lol) to ./archive\n",
    "\n",
    "\n",
    "------------- ./ScriptAssignment2.ipynb\n",
    "\n",
    "------------- ./archive_usfinancial/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')  # one time execution if not cached yet\n",
    "nltk.download('stopwords')  # one time execution if not cached yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "      \n",
    "def read_data(archives_dir):\n",
    "    corpus = []\n",
    "    counter = 0\n",
    "    # os.walk gets all the folders and paths in given directory\n",
    "    for root, dirs, files in os.walk(archives_dir):\n",
    "        for i, file in enumerate(files):\n",
    "            # join path(root) and filename so that this can be computer-independent as long as names are the same\n",
    "            # open in readonly mode, need encoding and ignore errors for weird characters in docs\n",
    "            with open(os.path.join(root, file), 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "                # load json, can use strings for index from json tree\n",
    "                # if data you want is inside of some structure, can use index like ['test1']['title']\n",
    "                # https://devqa.io/python-parse-json/\n",
    "                \n",
    "                data = json.load(f)\n",
    "                appendme = data['title'] + \" \" + data['published'] + \" \" + data['text'] # Create string with title, publish date, and document text\n",
    "                \n",
    "                if appendme: # Make sure not empty string\n",
    "                    corpus.append((appendme))\n",
    "            \n",
    "            # This is here to only go through a few documents instead of the 2GB of documents\n",
    "            # Leave in place to speed up testing/debug\n",
    "            # Uncomment for real data/tests\n",
    "            counter += 1\n",
    "            if counter > 100: # Break because a lot of documents\n",
    "                return corpus\n",
    "            \n",
    "# Extracted the articles into folder called \"archive_usfinancial\", change as needed\n",
    "# read_data() will parse through all folders/files inside there\n",
    "\n",
    "######################################################\n",
    "########### WARNING: There are a lot of files in there and it took an hour and a half to extract on my desktop\n",
    "######################################################\n",
    "corpus = read_data(os.getcwd() + \"\\\\archive_usfinancial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'emerging',\n",
       " 'market',\n",
       " 'etf',\n",
       " 'eem',\n",
       " 'was',\n",
       " 'a',\n",
       " 'weak',\n",
       " 'dollar',\n",
       " 'and']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    regex = re.compile('[^a-zA-Z0-9]')  # regex to only match alphanumeric characters\n",
    "    tokens = []\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(corpus):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            clean_word = regex.sub('', word)\n",
    "            if clean_word:  # make sure we're not appending empty strings\n",
    "                tokens.append(clean_word.lower())\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(corpus[0])\n",
    "tokens[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charts',\n",
       " 'dollartracking',\n",
       " 'etf',\n",
       " 'uup',\n",
       " 'believes',\n",
       " 'us',\n",
       " 'currency',\n",
       " 'continue',\n",
       " 'help',\n",
       " 'group']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "no_stopwords = remove_stopwords(tokens)\n",
    "no_stopwords[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chart',\n",
       " 'dollartrack',\n",
       " 'etf',\n",
       " 'uup',\n",
       " 'believ',\n",
       " 'us',\n",
       " 'currenc',\n",
       " 'continu',\n",
       " 'help',\n",
       " 'group']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(word):\n",
    "    return stemmer.stem(word).strip()\n",
    "\n",
    "stemmed = [stem_words(word) for word in no_stopwords]\n",
    "stemmed[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Re-do the tfid vectorizer with the preprocessing as a single function\n",
    "# @TODO: Maybe it's not needed though\n",
    "# @TODO: same todo as below\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# def preprocess_corpus(corpus):\n",
    "#     # Tokenize\n",
    "#     regex = re.compile('[^a-zA-Z0-9]')  # regex to only match alphanumeric characters\n",
    "#     tokens = []\n",
    "    \n",
    "#     for sent in nltk.sent_tokenize(corpus):\n",
    "#         for word in nltk.word_tokenize(sent):\n",
    "#             clean_word = regex.sub('', word)\n",
    "#             if clean_word:  # make sure we're not appending empty strings\n",
    "#                 tokens.append(clean_word.lower())\n",
    "\n",
    "#     print(tokens[50:60]) # Print for debug\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stopwords = nltk.corpus.stopwords.words('english')\n",
    "#     no_stopwords = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "#     print(no_stopwords[50:60]) # Print for debug\n",
    "\n",
    "#     # Stemming\n",
    "#     stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#     stemmed = [stemmer.stem(word).strip()for word in no_stopwords]\n",
    "#     print(stemmed[50:60]) # Print for debug\n",
    "        \n",
    "#     return stemmed\n",
    "     \n",
    "# processed_data = preprocess_corpus(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick_\\Documents\\MSCpE\\CAP6640 Natural Language Processing\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\nick_\\Documents\\MSCpE\\CAP6640 Natural Language Processing\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.95, max_features=50000,\n",
    "                        min_df=0.0, stop_words='english',\n",
    "                        use_idf=True, tokenizer=tokenize,\n",
    "                        preprocessor=stem_words, lowercase=True)\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Re-do the tfid vectorizer with the preprocessing as a single function\n",
    "# @TODO: Maybe it's not needed though\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf2 = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "#                         min_df=0.05, stop_words='english',\n",
    "#                         use_idf=True,\n",
    "#                         preprocessor=preprocess_corpus, lowercase=True)\n",
    "# tfidf_matrix2 = tfidf2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "['peso', '1', 'moneygram', 'company', 'stock', 'percent', 'said', 'exits', 'billion', 's']\n",
      "\n",
      "\n",
      "Topic #1:\n",
      "['futures', 'korea', 'macy', '2018', 'bitcoin', 'tax', 'said', 'year', 'percent', 's']\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "['court', '2', 'iran', 'caterpillar', 'google', 'bitcoin', 'said', 'amazon', 'china', 's']\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "['immigration', 'copper', 'ripple', 'commodity', 'said', 'gordon', '2018', '0054', 'prices', 's']\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "['jones', 'cuban', 'tech', 'stocks', 'says', 'said', 'china', 'trump', 'cramer', 's']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @TODO: Need to reformat and comment/write this better for readability and understanding\n",
    "# @TODO: Also need to figure out what kind of results/metrics we can get out of this and report\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    print(f'Topic #{i}:')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
