{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 2 - Fake News Detection\n",
    "### Mar 28, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "# Importing Libraries w.r.t Word Embedding layer & Lstm NN\n",
    "from tensorflow.keras.layers import Embedding, Hashing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training dataset as a dataframe\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Drop NaN rows from the dataframe to avoid errors\n",
    "df = df.dropna(how='all', axis=0)\n",
    "\n",
    "# Merge the author and title into a single column\n",
    "df['article'] = df['author'] + ' ' + df['title']\n",
    "# print(df['article'])\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>Darrell Lucus House Dem Aide: We Didn’t Even S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Consortiumnews.com Why the Truth Might Get You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jessica Purkiss 15 Civilians Killed In Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Howard Portnoy Iranian woman jailed for fictio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                             article  \n",
       "0  Darrell Lucus House Dem Aide: We Didn’t Even S...  \n",
       "1  Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
       "2  Consortiumnews.com Why the Truth Might Get You...  \n",
       "3  Jessica Purkiss 15 Civilians Killed In Single ...  \n",
       "4  Howard Portnoy Iranian woman jailed for fictio...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Combine title and author into a single column, diverting from tutorial\n",
    "# df = df.drop('title', axis=1)\n",
    "# df = df.drop('author', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in into data (X) and labels (Y)\n",
    "\n",
    "# Seclude the labels\n",
    "# 1 - unreliable\n",
    "# 0 - reliable\n",
    "labels = df['label']\n",
    "#print(labels)\n",
    "\n",
    "# Get the data excluding the labels\n",
    "data = df.drop('label', axis=1)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This only lemmatises the article feature, not the text found in the article - figure out if we have to do this\n",
    "\n",
    "# Init lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_corpus = []\n",
    "\n",
    "\n",
    "if os.path.isfile('cleaned_corpus.txt'):\n",
    "    with open('cleaned_corpus.txt', 'r') as f:\n",
    "        cleaned_corpus = [line.rstrip() for line in f]\n",
    "else:\n",
    "    with open('cleaned_corpus.txt', 'w') as f:\n",
    "        # For each article (author + title) in the dataframe, clean the text\n",
    "        for i in range(len(df)):\n",
    "            # Remove any non-alphanumeric characters\n",
    "            cleaned_article = re.sub('[^a-zA-Z0-9]', ' ', str(df['article'][i]))\n",
    "            # Lowercase the text\n",
    "            cleaned_article = cleaned_article.lower()\n",
    "            # Split the text into individual words\n",
    "            cleaned_article = cleaned_article.split()\n",
    "            # Lemmatize the words and remove any stopwords\n",
    "            cleaned_article = [lemmatizer.lemmatize(word) for word in cleaned_article if not word in set(stopwords.words('english'))]\n",
    "            # Join the words back together\n",
    "            cleaned_article = ' '.join(cleaned_article)\n",
    "            cleaned_corpus.append(cleaned_article)\n",
    "            f.write(\"%s\\n\" % cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any lables associated with empty articles\n",
    "labels = df[list(map(lambda x: len(x) > 0, cleaned_corpus))]\n",
    "labels = labels['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20771"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove these empty articles from the corpus \n",
    "cleaned_corpus = [i for i in cleaned_corpus if i]\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 51\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary - one hot encoding will assign the words with a number in range of 0 to vocab_size\n",
    "vocab_size = 5000\n",
    "\n",
    "# One hot encoding the cleaned corpus\n",
    "onehot_repr=[one_hot(words, vocab_size) for words in cleaned_corpus]\n",
    "\n",
    "# Finding the Max_len & Min_len of the sentences in the cleaned corpus\n",
    "def get_min_max_lengths(cleaned_corpus):\n",
    "    # Arbitrary values to compare against\n",
    "    min_len = float('inf')\n",
    "    max_len = float('-inf')\n",
    "\n",
    "    # Iterate through the corpus to get the max and min length sentences\n",
    "    for i in cleaned_corpus:\n",
    "        if len(i.split()) > max_len:\n",
    "            max_len = len(i.split())\n",
    "        if len(i.split()) < min_len:\n",
    "            min_len = len(i.split())\n",
    "    return min_len, max_len \n",
    "\n",
    "min_len, max_len = get_min_max_lengths(cleaned_corpus)\n",
    "print(min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sentences with zeroes at the start to make them all the same length\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 50 features\n",
    "embedding_vector_features = 50 ## feature representation\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features))\n",
    "model.add(LSTM(100)) # 100 Neurons - Hyper-Parameter for LSTM\n",
    "model.add(Dense(1, activation='sigmoid')) # Output Layer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embedded_docs)\n",
    "Y = np.array(labels)\n",
    "\n",
    "# Train & Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8946 - loss: 0.2448 - val_accuracy: 0.9867 - val_loss: 0.0518\n",
      "Epoch 2/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9934 - loss: 0.0256 - val_accuracy: 0.9860 - val_loss: 0.0518\n",
      "Epoch 3/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9982 - loss: 0.0087 - val_accuracy: 0.9889 - val_loss: 0.0552\n",
      "Epoch 4/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.0102 - val_accuracy: 0.9886 - val_loss: 0.0517\n",
      "Epoch 5/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0084 - val_accuracy: 0.9806 - val_loss: 0.0681\n",
      "Epoch 6/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0056 - val_accuracy: 0.9865 - val_loss: 0.0733\n",
      "Epoch 7/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0062 - val_accuracy: 0.9881 - val_loss: 0.0509\n",
      "Epoch 8/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0069 - val_accuracy: 0.9872 - val_loss: 0.0634\n",
      "Epoch 9/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0043 - val_accuracy: 0.9884 - val_loss: 0.0650\n",
      "Epoch 10/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0062 - val_accuracy: 0.9896 - val_loss: 0.0635\n",
      "Epoch 11/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9901 - val_loss: 0.0619\n",
      "Epoch 12/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0059 - val_accuracy: 0.9910 - val_loss: 0.0623\n",
      "Epoch 13/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0069 - val_accuracy: 0.9905 - val_loss: 0.0674\n",
      "Epoch 14/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0063 - val_accuracy: 0.9840 - val_loss: 0.0717\n",
      "Epoch 15/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0186 - val_accuracy: 0.9897 - val_loss: 0.0407\n",
      "Epoch 16/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0071 - val_accuracy: 0.9910 - val_loss: 0.0424\n",
      "Epoch 17/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9907 - val_loss: 0.0454\n",
      "Epoch 18/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.9907 - val_loss: 0.0479\n",
      "Epoch 19/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0061 - val_accuracy: 0.9907 - val_loss: 0.0507\n",
      "Epoch 20/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0043 - val_accuracy: 0.9902 - val_loss: 0.0538\n",
      "Epoch 21/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9901 - val_loss: 0.0559\n",
      "Epoch 22/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9901 - val_loss: 0.0597\n",
      "Epoch 23/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.9897 - val_loss: 0.0634\n",
      "Epoch 24/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0070 - val_accuracy: 0.9899 - val_loss: 0.0643\n",
      "Epoch 25/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0075 - val_accuracy: 0.9892 - val_loss: 0.0687\n",
      "Epoch 26/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.9894 - val_loss: 0.0713\n",
      "Epoch 27/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0061 - val_accuracy: 0.9902 - val_loss: 0.0724\n",
      "Epoch 28/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0065 - val_accuracy: 0.9889 - val_loss: 0.0784\n",
      "Epoch 29/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0064 - val_accuracy: 0.9896 - val_loss: 0.0809\n",
      "Epoch 30/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0070 - val_accuracy: 0.9897 - val_loss: 0.0836\n",
      "Epoch 31/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.9894 - val_loss: 0.0871\n",
      "Epoch 32/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9986 - loss: 0.0079 - val_accuracy: 0.9894 - val_loss: 0.0906\n",
      "Epoch 33/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0058 - val_accuracy: 0.9892 - val_loss: 0.0943\n",
      "Epoch 34/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9886 - val_loss: 0.0980\n",
      "Epoch 35/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.9888 - val_loss: 0.1003\n",
      "Epoch 36/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0071 - val_accuracy: 0.9888 - val_loss: 0.1044\n",
      "Epoch 37/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9896 - val_loss: 0.1059\n",
      "Epoch 38/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0051 - val_accuracy: 0.9891 - val_loss: 0.1100\n",
      "Epoch 39/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9986 - loss: 0.0076 - val_accuracy: 0.9896 - val_loss: 0.1113\n",
      "Epoch 40/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0056 - val_accuracy: 0.9896 - val_loss: 0.1138\n",
      "Epoch 41/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0045 - val_accuracy: 0.9892 - val_loss: 0.1162\n",
      "Epoch 42/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9896 - val_loss: 0.1178\n",
      "Epoch 43/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0072 - val_accuracy: 0.9892 - val_loss: 0.1199\n",
      "Epoch 44/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0052 - val_accuracy: 0.9892 - val_loss: 0.1218\n",
      "Epoch 45/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0046 - val_accuracy: 0.9892 - val_loss: 0.1227\n",
      "Epoch 46/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9894 - val_loss: 0.1211\n",
      "Epoch 47/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0074 - val_accuracy: 0.9889 - val_loss: 0.1261\n",
      "Epoch 48/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0061 - val_accuracy: 0.9894 - val_loss: 0.1278\n",
      "Epoch 49/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0044 - val_accuracy: 0.9894 - val_loss: 0.1219\n",
      "Epoch 50/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9886 - val_loss: 0.1344\n",
      "Epoch 51/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9986 - loss: 0.0079 - val_accuracy: 0.9878 - val_loss: 0.1402\n",
      "Epoch 52/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.9894 - val_loss: 0.1223\n",
      "Epoch 53/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9986 - loss: 0.0075 - val_accuracy: 0.9883 - val_loss: 0.1392\n",
      "Epoch 54/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9980 - loss: 0.0104 - val_accuracy: 0.9880 - val_loss: 0.1468\n",
      "Epoch 55/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9891 - val_loss: 0.1311\n",
      "Epoch 56/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0044 - val_accuracy: 0.9888 - val_loss: 0.1287\n",
      "Epoch 57/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0087 - val_accuracy: 0.9881 - val_loss: 0.1406\n",
      "Epoch 58/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0063 - val_accuracy: 0.9880 - val_loss: 0.1450\n",
      "Epoch 59/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0042 - val_accuracy: 0.9883 - val_loss: 0.1335\n",
      "Epoch 60/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9983 - loss: 0.0094 - val_accuracy: 0.9910 - val_loss: 0.0575\n",
      "Epoch 61/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9977 - loss: 0.0131 - val_accuracy: 0.9846 - val_loss: 0.0624\n",
      "Epoch 62/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9981 - loss: 0.0087 - val_accuracy: 0.9915 - val_loss: 0.0403\n",
      "Epoch 63/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9985 - loss: 0.0077 - val_accuracy: 0.9907 - val_loss: 0.0421\n",
      "Epoch 64/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9980 - loss: 0.0104 - val_accuracy: 0.9910 - val_loss: 0.0441\n",
      "Epoch 65/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.9912 - val_loss: 0.0459\n",
      "Epoch 66/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0042 - val_accuracy: 0.9909 - val_loss: 0.0479\n",
      "Epoch 67/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.9907 - val_loss: 0.0500\n",
      "Epoch 68/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0063 - val_accuracy: 0.9905 - val_loss: 0.0518\n",
      "Epoch 69/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0046 - val_accuracy: 0.9905 - val_loss: 0.0542\n",
      "Epoch 70/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0054 - val_accuracy: 0.9902 - val_loss: 0.0563\n",
      "Epoch 71/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0068 - val_accuracy: 0.9897 - val_loss: 0.0584\n",
      "Epoch 72/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0068 - val_accuracy: 0.9901 - val_loss: 0.0599\n",
      "Epoch 73/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0069 - val_accuracy: 0.9897 - val_loss: 0.0623\n",
      "Epoch 74/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0070 - val_accuracy: 0.9897 - val_loss: 0.0647\n",
      "Epoch 75/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0072 - val_accuracy: 0.9892 - val_loss: 0.0677\n",
      "Epoch 76/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0054 - val_accuracy: 0.9889 - val_loss: 0.0705\n",
      "Epoch 77/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0042 - val_accuracy: 0.9886 - val_loss: 0.0738\n",
      "Epoch 78/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0064 - val_accuracy: 0.9888 - val_loss: 0.0758\n",
      "Epoch 79/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0059 - val_accuracy: 0.9888 - val_loss: 0.0776\n",
      "Epoch 80/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0056 - val_accuracy: 0.9886 - val_loss: 0.0798\n",
      "Epoch 81/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.9886 - val_loss: 0.0817\n",
      "Epoch 82/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.9888 - val_loss: 0.0831\n",
      "Epoch 83/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0039 - val_accuracy: 0.9888 - val_loss: 0.0843\n",
      "Epoch 84/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9991 - loss: 0.0051 - val_accuracy: 0.9888 - val_loss: 0.0854\n",
      "Epoch 85/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0072 - val_accuracy: 0.9891 - val_loss: 0.0866\n",
      "Epoch 86/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0043 - val_accuracy: 0.9891 - val_loss: 0.0870\n",
      "Epoch 87/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0042 - val_accuracy: 0.9891 - val_loss: 0.0880\n",
      "Epoch 88/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0070 - val_accuracy: 0.9892 - val_loss: 0.0891\n",
      "Epoch 89/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0086 - val_accuracy: 0.9894 - val_loss: 0.0907\n",
      "Epoch 90/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.9897 - val_loss: 0.0894\n",
      "Epoch 91/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.9896 - val_loss: 0.0910\n",
      "Epoch 92/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0041 - val_accuracy: 0.9897 - val_loss: 0.0906\n",
      "Epoch 93/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0065 - val_accuracy: 0.9897 - val_loss: 0.0917\n",
      "Epoch 94/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0067 - val_accuracy: 0.9897 - val_loss: 0.0931\n",
      "Epoch 95/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9897 - val_loss: 0.0932\n",
      "Epoch 96/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0082 - val_accuracy: 0.9897 - val_loss: 0.0936\n",
      "Epoch 97/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.9897 - val_loss: 0.0938\n",
      "Epoch 98/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0068 - val_accuracy: 0.9897 - val_loss: 0.0944\n",
      "Epoch 99/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0053 - val_accuracy: 0.9896 - val_loss: 0.0949\n",
      "Epoch 100/100\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0034 - val_accuracy: 0.9896 - val_loss: 0.0952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d2d3502a80>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train da model\n",
    "\n",
    "if os.path.isfile('model_combined_author_title.keras'):\n",
    "    model = tf.keras.models.load_model('model_combined_author_title.keras')\n",
    "else:\n",
    "    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=20)\n",
    "# Training on CPU took 11m 37.4s - Nick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save da model\n",
    "\n",
    "if os.path.isfile('model_combined_author_title.keras'):\n",
    "    pass\n",
    "else:\n",
    "    model.save('model_combined_author_title.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
