{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 1 - Extractive Summarization\n",
    "### Feb 8, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, LsiModel\n",
    "from scipy import spatial\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On March 11, 2011, Japan experienced the strongest earthquake in its recorded history. The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu. The Tohoku earthquake caused a tsunami. A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water. Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions. The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami. More than 15,500 people died. The tsunami also severely crippled the infrastructure of the country.In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant. The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.\n"
     ]
    }
   ],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to a text file\n",
    "    Output: String of the text file\n",
    "'''\n",
    "def txt_file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_text = txt_file_to_string(document_filepath)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On March 11, 2011, Japan experienced the strongest earthquake in its recorded history.', 'The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu.', 'The Tohoku earthquake caused a tsunami.', 'A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water.', 'Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions.', 'The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami.', 'More than 15,500 people died.', 'The tsunami also severely crippled the infrastructure of the country.In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant.', 'The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.']\n",
      "['on march 11, 2011, japan experienced the strongest earthquake in its recorded history.', 'the earthquake struck below the north pacific, 130 kilometers (81 miles) east of sendai, the largest city in the tohoku region, a northern part of the island of honshu.', 'the tohoku earthquake caused a tsunami.', 'a tsunami—japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water.', 'most tsunamis, like the one that formed off tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions.', 'the tohoku tsunami produced waves up to 40 meters (132 feet) high, more than 450,000 people became homeless as a result of the tsunami.', 'more than 15,500 people died.', 'the tsunami also severely crippled the infrastructure of the country.in addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the fukushima daiichi nuclear power plant.', 'the fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.']\n",
      "['on march 11 2011 japan experienced the strongest earthquake in its recorded history', 'the earthquake struck below the north pacific 130 kilometers 81 miles east of sendai the largest city in the tohoku region a northern part of the island of honshu', 'the tohoku earthquake caused a tsunami', 'a tsunami—japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water', 'most tsunamis like the one that formed off tohoku are triggered by underwater tectonic activity such as earthquakes and volcanic eruptions', 'the tohoku tsunami produced waves up to 40 meters 132 feet high more than 450000 people became homeless as a result of the tsunami', 'more than 15500 people died', 'the tsunami also severely crippled the infrastructure of the countryin addition to the thousands of destroyed homes businesses roads and railways the tsunami caused the meltdown of three nuclear reactors at the fukushima daiichi nuclear power plant', 'the fukushima nuclear disaster released toxic radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses']\n",
      "[['march', '11', '2011', 'japan', 'experienced', 'strongest', 'earthquake', 'recorded', 'history'], ['earthquake', 'struck', 'north', 'pacific', '130', 'kilometers', '81', 'miles', 'east', 'sendai', 'largest', 'city', 'tohoku', 'region', 'northern', 'part', 'island', 'honshu'], ['tohoku', 'earthquake', 'caused', 'tsunami'], ['tsunami—japanese', '“harbor', 'wave”—is', 'series', 'powerful', 'waves', 'caused', 'displacement', 'large', 'body', 'water'], ['tsunamis', 'like', 'one', 'formed', 'tohoku', 'triggered', 'underwater', 'tectonic', 'activity', 'earthquakes', 'volcanic', 'eruptions'], ['tohoku', 'tsunami', 'produced', 'waves', '40', 'meters', '132', 'feet', 'high', '450000', 'people', 'became', 'homeless', 'result', 'tsunami'], ['15500', 'people', 'died'], ['tsunami', 'also', 'severely', 'crippled', 'infrastructure', 'countryin', 'addition', 'thousands', 'destroyed', 'homes', 'businesses', 'roads', 'railways', 'tsunami', 'caused', 'meltdown', 'three', 'nuclear', 'reactors', 'fukushima', 'daiichi', 'nuclear', 'power', 'plant'], ['fukushima', 'nuclear', 'disaster', 'released', 'toxic', 'radioactive', 'materials', 'environment', 'forced', 'thousands', 'people', 'evacuate', 'homes', 'businesses']]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: remove punctuation, tokenize, and remove stopwords\n",
    "\n",
    "'''\n",
    "    Purpose: Perform appropriate preprocessing on the text file for the TextRank algorithm\n",
    "'''\n",
    "def preprocess_text(text, stop_words):\n",
    "    tokenized_sentences = sent_tokenize(text, language='english')\n",
    "    print(tokenized_sentences)\n",
    "\n",
    "    sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "    print(sentences_to_lower)\n",
    "\n",
    "    # Regular Expression to match any punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Remove the punctuation from the lowercase sentences\n",
    "    sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "    print(sentences_no_punctuation)\n",
    "\n",
    "    data = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_no_punctuation]\n",
    "    return data, tokenized_sentences\n",
    "\n",
    "# Obtain stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Preprocess the text to obtain the data we will use going forward\n",
    "data, tokenized_sentences = preprocess_text(document_text, stop_words)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.12973852 -0.25991294 -0.18357903 -0.31982276  0.43648082\n",
      "   0.19730724 -0.27834651 -0.19436854]\n",
      " [ 0.12973852  1.         -0.10913887 -0.33759162  0.01915257 -0.30675822\n",
      "   0.12131891  0.19208398 -0.07006265]\n",
      " [-0.25991294 -0.10913887  1.          0.37436241  0.36843267 -0.0707379\n",
      "  -0.67658389  0.23411885 -0.34417024]\n",
      " [-0.18357903 -0.33759162  0.37436241  1.          0.25514823 -0.25728783\n",
      "  -0.38001704 -0.10238185  0.03989816]\n",
      " [-0.31982276  0.01915257  0.36843267  0.25514823  1.         -0.48927537\n",
      "  -0.11403655  0.26633519  0.11107261]\n",
      " [ 0.43648082 -0.30675822 -0.0707379  -0.25728783 -0.48927537  1.\n",
      "   0.02898987 -0.33324486  0.11546853]\n",
      " [ 0.19730724  0.12131891 -0.67658389 -0.38001704 -0.11403655  0.02898987\n",
      "   1.         -0.27139169  0.47717941]\n",
      " [-0.27834651  0.19208398  0.23411885 -0.10238185  0.26633519 -0.33324486\n",
      "  -0.27139169  1.         -0.25321361]\n",
      " [-0.19436854 -0.07006265 -0.34417024  0.03989816  0.11107261  0.11546853\n",
      "   0.47717941 -0.25321361  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: Word Embeddings \n",
    " \n",
    "# Grab the maximum number of words in a sentence for padding sentence embeddings\n",
    "max_sentence_length = max([len(sentence) for sentence in data])\n",
    "\n",
    "'''\n",
    "    Train the Word2Vec model on the data and calculate embeddings for each word\n",
    "        min_count: Ignores all words with total frequency lower than this\n",
    "        vector_size: Dimensionality of the word vectors\n",
    "'''\n",
    "# NOTE: If output is unsatsifactory, train for longer epochs\n",
    "model = Word2Vec(data, min_count=1, vector_size=1)\n",
    "\n",
    "# Grab sentence embeddings by leveraging the word embeddings and sentence tokens\n",
    "sentence_embeddings = [[model.wv[word][0] for word in words] for words in data]\n",
    "\n",
    "# Pad the sentence embeddings with 0's to ensure all sentences have the same length\n",
    "sentence_embeddings = [np.pad(embedding, (0, max_sentence_length - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "# Calculate the similarity matrix\n",
    "# Instantiate a matrix of zeros with the same shape as the number of sentences\n",
    "similarity_matrix = np.zeros([len(data), len(data)])\n",
    "\n",
    "# Populate the similarity matrix with cosine similarity scores (same as 1 - cosine distance)\n",
    "for i, row in enumerate(sentence_embeddings):\n",
    "    for j, col in enumerate(sentence_embeddings):\n",
    "        similarity_matrix[i][j] = 1 - spatial.distance.cosine(row, col)\n",
    "\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:497: RuntimeWarning: invalid value encountered in subtract\n",
      "  err = np.absolute(x - xlast).sum()\n"
     ]
    },
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 5000 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Convert similarity matrix to an nx graph and call nx's pagerank\u001b[39;00m\n\u001b[0;32m     17\u001b[0m graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mfrom_numpy_array(similarity_matrix)\n\u001b[1;32m---> 18\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpagerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# NOTE: Modify this variable to change the number of sentences in the summary\u001b[39;00m\n\u001b[0;32m     21\u001b[0m num_sent_to_extract \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\networkx\\classes\\backends.py:148\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[0;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m             )\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:110\u001b[0m, in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpagerank\u001b[39m(\n\u001b[0;32m     11\u001b[0m     G,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     dangling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the PageRank of the nodes in the graph.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    PageRank computes a ranking of the nodes in the graph G based on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pagerank_scipy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdangling\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:500\u001b[0m, in \u001b[0;36m_pagerank_scipy\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m<\u001b[39m N \u001b[38;5;241m*\u001b[39m tol:\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nodelist, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, x)))\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mPowerIterationFailedConvergence(max_iter)\n",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 5000 iterations')"
     ]
    }
   ],
   "source": [
    "# TextRank: Call nx's pagerank to get scores. \n",
    "\n",
    "''' \n",
    "    Get the top n sentences from pagerank scores\n",
    "'''\n",
    "def top_n_sentences(n, scores, tokenized_sentences):\n",
    "    # Key => Sentence \n",
    "    # Value => PageRank Score\n",
    "    sentence_score_dict = {sentence:scores[i] for i, sentence in enumerate(tokenized_sentences)}\n",
    "\n",
    "    # Filter the dictionary to contain only the top n sentences\n",
    "    top_sentences = dict(sorted(sentence_score_dict.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "# Convert similarity matrix to an nx graph and call nx's pagerank\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(graph, max_iter=5000)\n",
    "\n",
    "# NOTE: Modify this variable to change the number of sentences in the summary\n",
    "num_sent_to_extract = 3\n",
    "\n",
    "extractive_summary = top_n_sentences(num_sent_to_extract, scores, tokenized_sentences)\n",
    "\n",
    "# Iterate through the dictionary to output the summary\n",
    "for sentence, score in extractive_summary.items():\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluation will depend on the method used to implement extractive summarization\n",
    "#       - ILP (Integer Linear Programming): We can use ROUGE-2 for evaluation\n",
    "# Andres NOTE: This is the only section that I am unsure of. It would be cool to use ROUGE-2 to compare our TextRank algorithm to the bigram inspection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# LSI (Latent Sentiment Indexing): Multi-document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Tokenize, remove stopwords, and stem the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Term Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Create LSI Model using Gensim\n",
    "\n",
    "# Sort documents by weight \n",
    "\n",
    "# Sort vectors by score \n",
    "\n",
    "# Select top documents \n",
    "\n",
    "# Sort sentence numbers in order \n",
    "\n",
    "# Obtain the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "##### The following tutorials helped us implement the algorithms in the document:\n",
    "##### 1. https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n",
    "##### 2. https://towardsdatascience.com/document-summarization-using-latent-semantic-indexing-b747ef2d2af6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
