{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 1 - Extractive Summarization\n",
    "### Feb 8, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On March 11, 2011, Japan experienced the strongest earthquake in its recorded history. The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu. The Tohoku earthquake caused a tsunami. A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water. Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions. The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami. More than 15,500 people died. The tsunami also severely crippled the infrastructure of the country.In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant. The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.\n"
     ]
    }
   ],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# -LSI (Latent Sentiment Indexing): Multi-document summarization\n",
    "# -ILP (Integer Linear Programming): Multi-document summarization is \"easier\" and performs better\n",
    "# -TextRank: Single-document summarization\n",
    "# Andres NOTE: I believe the TextRank algorithm is the most appropriate for this task\n",
    "\n",
    "'''\n",
    "    Input: File path to a text file\n",
    "    Output: String of the text file\n",
    "'''\n",
    "def txt_file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_text = txt_file_to_string(document_filepath)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence_in_tokens\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Obtain stopwords from nltk\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Preprocess the text to obtain the data we will use going forward\u001b[39;00m\n\u001b[0;32m     28\u001b[0m data \u001b[38;5;241m=\u001b[39m preprocess_text(document_text, stopwords)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# NOTE: This step depends on the method we want to use to perform extractive summarization \n",
    "#       - LSI (Latent Sentiment Indexing): Tokenize, remove stopwords, and stem the words\n",
    "#       - ILP (Integer Linear Programming): Bigram based method so we want to EXTRACT STOPWORDS to ensure that the bigrams are meaningful\n",
    "#       - TextRank: remove punctuation, tokenize, and remove stopwords\n",
    "\n",
    "'''\n",
    "    Purpose: Perform appropriate preprocessing on the text file for the TextRank algorithm\n",
    "'''\n",
    "def preprocess_text(text, stopwords):\n",
    "    tokenized_sentences = sent_tokenize(text, language='english')\n",
    "    print(tokenized_sentences)\n",
    "\n",
    "    sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "    print(sentences_to_lower)\n",
    "\n",
    "    # Regular Expression to match any punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Remove the punctuation from the lowercase sentences\n",
    "    sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "    print(sentences_no_punctuation)\n",
    "\n",
    "    sentence_in_tokens = [[words for words in sentence.split(' ') if words not in stopwords] for sentence in sentences_no_punctuation]\n",
    "    return sentence_in_tokens\n",
    "\n",
    "# Obtain stopwords from nltk\n",
    "stopwords = stopwords\n",
    "# Preprocess the text to obtain the data we will use going forward\n",
    "data = preprocess_text(document_text, stopwords)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - TextRank: Word Embeddings \n",
    "# TODO: Take preprocessed data and pass it to Word2Vec to calculate word and sentence embeddings \n",
    "\n",
    "# TODO: Calculate the similarity matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - TextRank: Call nx's pagerank to get scores. \n",
    "# TODO: Make a method that takes the top n scores from the scores variable and grabs the corresponding n sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluation will depend on the method used to implement extractive summarization\n",
    "#       - ILP (Integer Linear Programming): We can use ROUGE-2 for evaluation\n",
    "# Andres NOTE: This is the only section that I am unsure of. It would be cool to use ROUGE-2 to compare our TextRank algorithm to the bigram inspection \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
