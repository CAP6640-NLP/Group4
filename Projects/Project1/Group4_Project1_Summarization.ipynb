{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 1 - Extractive Summarization\n",
    "### Feb 8, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, LsiModel\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On March 11, 2011, Japan experienced the strongest earthquake in its recorded history. The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu. The Tohoku earthquake caused a tsunami. A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water. Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions. The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami. More than 15,500 people died. The tsunami also severely crippled the infrastructure of the country.In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant. The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.\n"
     ]
    }
   ],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to a text file\n",
    "    Output: String of the text file\n",
    "'''\n",
    "def txt_file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_text = txt_file_to_string(document_filepath)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On March 11, 2011, Japan experienced the strongest earthquake in its recorded history.', 'The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu.', 'The Tohoku earthquake caused a tsunami.', 'A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water.', 'Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions.', 'The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami.', 'More than 15,500 people died.', 'The tsunami also severely crippled the infrastructure of the country.In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant.', 'The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.']\n",
      "['on march 11, 2011, japan experienced the strongest earthquake in its recorded history.', 'the earthquake struck below the north pacific, 130 kilometers (81 miles) east of sendai, the largest city in the tohoku region, a northern part of the island of honshu.', 'the tohoku earthquake caused a tsunami.', 'a tsunami—japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water.', 'most tsunamis, like the one that formed off tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions.', 'the tohoku tsunami produced waves up to 40 meters (132 feet) high, more than 450,000 people became homeless as a result of the tsunami.', 'more than 15,500 people died.', 'the tsunami also severely crippled the infrastructure of the country.in addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the fukushima daiichi nuclear power plant.', 'the fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.']\n",
      "['on march 11 2011 japan experienced the strongest earthquake in its recorded history', 'the earthquake struck below the north pacific 130 kilometers 81 miles east of sendai the largest city in the tohoku region a northern part of the island of honshu', 'the tohoku earthquake caused a tsunami', 'a tsunami—japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water', 'most tsunamis like the one that formed off tohoku are triggered by underwater tectonic activity such as earthquakes and volcanic eruptions', 'the tohoku tsunami produced waves up to 40 meters 132 feet high more than 450000 people became homeless as a result of the tsunami', 'more than 15500 people died', 'the tsunami also severely crippled the infrastructure of the countryin addition to the thousands of destroyed homes businesses roads and railways the tsunami caused the meltdown of three nuclear reactors at the fukushima daiichi nuclear power plant', 'the fukushima nuclear disaster released toxic radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses']\n",
      "[['march', '11', '2011', 'japan', 'experienced', 'strongest', 'earthquake', 'recorded', 'history'], ['earthquake', 'struck', 'north', 'pacific', '130', 'kilometers', '81', 'miles', 'east', 'sendai', 'largest', 'city', 'tohoku', 'region', 'northern', 'part', 'island', 'honshu'], ['tohoku', 'earthquake', 'caused', 'tsunami'], ['tsunami—japanese', '“harbor', 'wave”—is', 'series', 'powerful', 'waves', 'caused', 'displacement', 'large', 'body', 'water'], ['tsunamis', 'like', 'one', 'formed', 'tohoku', 'triggered', 'underwater', 'tectonic', 'activity', 'earthquakes', 'volcanic', 'eruptions'], ['tohoku', 'tsunami', 'produced', 'waves', '40', 'meters', '132', 'feet', 'high', '450000', 'people', 'became', 'homeless', 'result', 'tsunami'], ['15500', 'people', 'died'], ['tsunami', 'also', 'severely', 'crippled', 'infrastructure', 'countryin', 'addition', 'thousands', 'destroyed', 'homes', 'businesses', 'roads', 'railways', 'tsunami', 'caused', 'meltdown', 'three', 'nuclear', 'reactors', 'fukushima', 'daiichi', 'nuclear', 'power', 'plant'], ['fukushima', 'nuclear', 'disaster', 'released', 'toxic', 'radioactive', 'materials', 'environment', 'forced', 'thousands', 'people', 'evacuate', 'homes', 'businesses']]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: remove punctuation, tokenize, and remove stopwords\n",
    "\n",
    "'''\n",
    "    Purpose: Perform appropriate preprocessing on the text file for the TextRank algorithm\n",
    "'''\n",
    "def preprocess_text(text, stopwords):\n",
    "    tokenized_sentences = sent_tokenize(text, language='english')\n",
    "    print(tokenized_sentences)\n",
    "\n",
    "    sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "    print(sentences_to_lower)\n",
    "\n",
    "    # Regular Expression to match any punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Remove the punctuation from the lowercase sentences\n",
    "    sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "    print(sentences_no_punctuation)\n",
    "\n",
    "    sentence_in_tokens = [[words for words in sentence.split(' ') if words not in stopwords] for sentence in sentences_no_punctuation]\n",
    "    return sentence_in_tokens\n",
    "\n",
    "# Obtain stopwords from nltk\n",
    "stopwords = stopwords\n",
    "# Preprocess the text to obtain the data we will use going forward\n",
    "data = preprocess_text(document_text, stopwords)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank: Word Embeddings \n",
    "# TODO: Take preprocessed data and pass it to Word2Vec to calculate word and sentence embeddings \n",
    "\n",
    "# TODO: Calculate the similarity matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank: Call nx's pagerank to get scores. \n",
    "# TODO: Make a method that takes the top n scores from the scores variable and grabs the corresponding n sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluation will depend on the method used to implement extractive summarization\n",
    "#       - ILP (Integer Linear Programming): We can use ROUGE-2 for evaluation\n",
    "# Andres NOTE: This is the only section that I am unsure of. It would be cool to use ROUGE-2 to compare our TextRank algorithm to the bigram inspection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# LSI (Latent Sentiment Indexing): Multi-document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Tokenize, remove stopwords, and stem the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Term Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Create LSI Model using Gensim\n",
    "\n",
    "# Sort documents by weight \n",
    "\n",
    "# Sort vectors by score \n",
    "\n",
    "# Select top documents \n",
    "\n",
    "# Sort sentence numbers in order \n",
    "\n",
    "# Obtain the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "##### The following tutorials helped us implement the algorithms in the document:\n",
    "##### 1. https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n",
    "##### 2. https://towardsdatascience.com/document-summarization-using-latent-semantic-indexing-b747ef2d2af6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
