{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 1 - Extractive Summarization\n",
    "### Feb 8, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zlyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zlyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, LsiModel\n",
    "from gensim import corpora\n",
    "from scipy import spatial\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On March 11, 2011, Japan experienced the strongest earthquake in its recorded history. The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu. The Tohoku earthquake caused a tsunami. A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water. Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions. The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami. More than 15,500 people died. The tsunami also severely crippled the infrastructure of the country. In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant. The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.\n"
     ]
    }
   ],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to a text file\n",
    "    Output: String of the text file\n",
    "'''\n",
    "def txt_file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        data = file.read()\n",
    "        data = data.replace('\\n', ' ') # Remove newline characters\n",
    "    return data\n",
    "\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_text = txt_file_to_string(document_filepath)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['march', '11', '2011', 'japan', 'experienced', 'strongest', 'earthquake', 'recorded', 'history'], ['earthquake', 'struck', 'north', 'pacific', '130', 'kilometers', '81', 'miles', 'east', 'sendai', 'largest', 'city', 'tohoku', 'region', 'northern', 'part', 'island', 'honshu'], ['tohoku', 'earthquake', 'caused', 'tsunami'], ['tsunami—japanese', '“harbor', 'wave”—is', 'series', 'powerful', 'waves', 'caused', 'displacement', 'large', 'body', 'water'], ['tsunamis', 'like', 'one', 'formed', 'tohoku', 'triggered', 'underwater', 'tectonic', 'activity', 'earthquakes', 'volcanic', 'eruptions'], ['tohoku', 'tsunami', 'produced', 'waves', '40', 'meters', '132', 'feet', 'high', '450000', 'people', 'became', 'homeless', 'result', 'tsunami'], ['15500', 'people', 'died'], ['tsunami', 'also', 'severely', 'crippled', 'infrastructure', 'country'], ['addition', 'thousands', 'destroyed', 'homes', 'businesses', 'roads', 'railways', 'tsunami', 'caused', 'meltdown', 'three', 'nuclear', 'reactors', 'fukushima', 'daiichi', 'nuclear', 'power', 'plant'], ['fukushima', 'nuclear', 'disaster', 'released', 'toxic', 'radioactive', 'materials', 'environment', 'forced', 'thousands', 'people', 'evacuate', 'homes', 'businesses']]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: remove punctuation, tokenize, and remove stopwords\n",
    "\n",
    "'''\n",
    "    Purpose: Perform appropriate preprocessing on the text file for the TextRank algorithm\n",
    "'''\n",
    "def preprocess_text(text, stop_words):\n",
    "    tokenized_sentences = sent_tokenize(text, language='english')\n",
    "\n",
    "    sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "\n",
    "    # Regular Expression to match any punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Remove the punctuation from the lowercase sentences\n",
    "    sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "\n",
    "    data = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_no_punctuation]\n",
    "    return data, tokenized_sentences\n",
    "\n",
    "# Obtain stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Preprocess the text to obtain the data we will use going forward\n",
    "data, tokenized_sentences = preprocess_text(document_text, stop_words)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.70977305 0.6788758  0.90541902 0.85618414 0.77903019\n",
      "  0.58480879 0.82346425 0.70122287 0.81078446]\n",
      " [0.70977305 0.99999999 0.46256435 0.78552505 0.81960516 0.91196754\n",
      "  0.39324753 0.57507588 0.99913895 0.88168264]\n",
      " [0.6788758  0.46256435 0.99999998 0.61504091 0.56159318 0.52088329\n",
      "  0.8675951  0.81588685 0.47054226 0.54896826]\n",
      " [0.90541902 0.78552505 0.61504091 1.         0.95161855 0.85485302\n",
      "  0.52866716 0.74685248 0.77230259 0.89089428]\n",
      " [0.85618414 0.81960516 0.56159318 0.95161855 0.99999998 0.89274893\n",
      "  0.48374692 0.69104607 0.81084365 0.92575996]\n",
      " [0.77903019 0.91196754 0.52088329 0.85485302 0.89274893 0.99999998\n",
      "  0.44024469 0.63624942 0.90966395 0.96622476]\n",
      " [0.58480879 0.39324753 0.8675951  0.52866716 0.48374692 0.44024469\n",
      "  0.99999999 0.69989115 0.40420296 0.47373378]\n",
      " [0.82346425 0.57507588 0.81588685 0.74685248 0.69104607 0.63624942\n",
      "  0.69989115 0.99999999 0.57367478 0.66818031]\n",
      " [0.70122287 0.99913895 0.47054226 0.77230259 0.81084365 0.90966395\n",
      "  0.40420296 0.57367478 0.99999999 0.87944682]\n",
      " [0.81078446 0.88168264 0.54896826 0.89089428 0.92575996 0.96622476\n",
      "  0.47373378 0.66818031 0.87944682 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: Word Embeddings \n",
    " \n",
    "# Grab the maximum number of words in a sentence for padding sentence embeddings\n",
    "max_sentence_length = max([len(sentence) for sentence in data])\n",
    "\n",
    "'''\n",
    "    Train the Word2Vec model on the data and calculate embeddings for each word\n",
    "        min_count: Ignores all words with total frequency lower than this\n",
    "        vector_size: Dimensionality of the word vectors\n",
    "'''\n",
    "# NOTE: If output is unsatsifactory, train for longer epochs\n",
    "model = Word2Vec(data, min_count=1, vector_size=1, epochs=5000)\n",
    "\n",
    "# Grab sentence embeddings by leveraging the word embeddings and sentence tokens\n",
    "sentence_embeddings = [[model.wv[word][0] for word in words] for words in data]\n",
    "\n",
    "# Pad the sentence embeddings with 0's to ensure all sentences have the same length\n",
    "sentence_embeddings = [np.pad(embedding, (0, max_sentence_length - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "# Calculate the similarity matrix\n",
    "# Instantiate a matrix of zeros with the same shape as the number of sentences\n",
    "similarity_matrix = np.zeros([len(data), len(data)])\n",
    "\n",
    "# Populate the similarity matrix with cosine similarity scores (same as 1 - cosine distance)\n",
    "for i, row in enumerate(sentence_embeddings):\n",
    "    for j, col in enumerate(sentence_embeddings):\n",
    "        similarity_matrix[i][j] = 1 - spatial.distance.cosine(row, col)\n",
    "\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tsunami raced outward from the epicentre at speeds that approached about 500 miles (800 km) per hour.\n",
      "In addition to Sendai, other communities hard-hit by the tsunami included Kamaishi and Miyako in Iwate; Ishinomaki, Kesennuma, and Shiogama in Miyagi; and Kitaibaraki and Hitachinaka in Ibaraki.\n"
     ]
    }
   ],
   "source": [
    "# TextRank: Call nx's pagerank to get scores. \n",
    "\n",
    "''' \n",
    "    Get the top n sentences from pagerank scores\n",
    "'''\n",
    "def top_n_sentences(n, scores, tokenized_sentences):\n",
    "    # Key => Sentence \n",
    "    # Value => PageRank Score\n",
    "    sentence_score_dict = {sentence:scores[i] for i, sentence in enumerate(tokenized_sentences)}\n",
    "\n",
    "    # Filter the dictionary to contain only the top n sentences\n",
    "    top_sentences = dict(sorted(sentence_score_dict.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "# Convert similarity matrix to an nx graph and call nx's pagerank\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(graph)\n",
    "\n",
    "# NOTE: Modify this variable to change the number of sentences in the summary\n",
    "num_sent_to_extract = 2\n",
    "\n",
    "extractive_summary = top_n_sentences(num_sent_to_extract, scores, tokenized_sentences)\n",
    "\n",
    "# Iterate through the dictionary to output the summary\n",
    "for sentence, score in extractive_summary.items():\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluation will depend on the method used to implement extractive summarization\n",
    "#       - ILP (Integer Linear Programming): We can use ROUGE-2 for evaluation\n",
    "# Andres NOTE: This is the only section that I am unsure of. It would be cool to use ROUGE-2 to compare our TextRank algorithm to the bigram inspection\n",
    "\n",
    "\n",
    "def csv_column_to_list(file_path, column_index):\n",
    "    column_data = []\n",
    "    with open(file_path, encoding=\"utf8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if len(row) > column_index:  # Ensure the row has the desired column\n",
    "                column_data.append(row[column_index].replace(\"\\n\",\" \"))\n",
    "\n",
    "    return column_data\n",
    "\n",
    "csvFile = \"./Dataset/CnnTestData.csv\"\n",
    "\n",
    "# Get the list of articles and human summaries that we are going to be evaluating\n",
    "testDocs = csv_column_to_list(csvFile,1)\n",
    "testDocs = testDocs[1:21]\n",
    "\n",
    "humanSumm = csv_column_to_list(csvFile,2)\n",
    "humanSumm = humanSumm[1:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan's Misbah-ul-Haq (left) and Wahab Riaz look set to play international cricket in their homeland again .Security officials display arms and ammunition seized after the terrorists' attack on the Sri Lanka team .Sri Lankan cricketers were rescued by the Pakistani air force from Gadaffi Stadium after the terror attack .Pakistan appear set to host Test-playing opposition in their home country for the first time in more than six years.\n"
     ]
    }
   ],
   "source": [
    "# Get our models summarizations of the documents\n",
    "modelSumms = []\n",
    "\n",
    "for doc in testDocs:\n",
    "    data, tokenized_sentences = preprocess_text(doc, stop_words)\n",
    "    max_sentence_length = max([len(sentence) for sentence in data])\n",
    "    model = Word2Vec(data, min_count=1, vector_size=1, epochs=5000)\n",
    "\n",
    "    # Grab sentence embeddings by leveraging the word embeddings and sentence tokens\n",
    "    sentence_embeddings = [[model.wv[word][0] for word in words] for words in data]\n",
    "\n",
    "    # Pad the sentence embeddings with 0's to ensure all sentences have the same length\n",
    "    sentence_embeddings = [np.pad(embedding, (0, max_sentence_length - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "    # Calculate the similarity matrix\n",
    "    # Instantiate a matrix of zeros with the same shape as the number of sentences\n",
    "    similarity_matrix = np.zeros([len(data), len(data)])\n",
    "\n",
    "    # Populate the similarity matrix with cosine similarity scores (same as 1 - cosine distance)\n",
    "    for i, row in enumerate(sentence_embeddings):\n",
    "        for j, col in enumerate(sentence_embeddings):\n",
    "            similarity_matrix[i][j] = 1 - spatial.distance.cosine(row, col)\n",
    "\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(graph)\n",
    "    # NOTE: Modify this variable to change the number of sentences in the summary\n",
    "    num_sent_to_extract = 4\n",
    "\n",
    "    extractive_summary = top_n_sentences(num_sent_to_extract, scores, tokenized_sentences)\n",
    "\n",
    "    # Iterate through the dictionary to output the summary\n",
    "    s = \"\"\n",
    "    for sentence, score in extractive_summary.items():\n",
    "        s = s + sentence\n",
    "    \n",
    "    modelSumms.append(s)\n",
    "\n",
    "print(modelSumms[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3114754098360656, 0.25, 0.34532374100719426, 0.1806451612903226, 0.26277372262773724, 0.34328358208955223, 0.3793103448275862, 0.3783783783783784, 0.2096774193548387, 0.271604938271605, 0.411764705882353, 0.359375, 0.2987012987012987, 0.1889763779527559, 0.2967741935483871, 0.3595505617977528, 0.24778761061946902, 0.2580645161290323, 0.3793103448275862, 0.2894736842105263], [0.03333333333333333, 0.07936507936507936, 0.10218978102189781, 0.0261437908496732, 0.02962962962962963, 0.10606060606060606, 0.2280701754385965, 0.09589041095890412, 0.06557377049180328, 0.05, 0.04477611940298507, 0.07936507936507937, 0.09210526315789473, 0.015999999999999997, 0.05228758169934641, 0.11363636363636365, 0.036036036036036036, 0.09836065573770492, 0.10526315789473684, 0.03539823008849558], [0.1639344262295082, 0.18750000000000003, 0.2158273381294964, 0.0903225806451613, 0.13138686131386862, 0.23880597014925375, 0.2586206896551724, 0.22972972972972974, 0.12903225806451613, 0.1358024691358025, 0.17647058823529413, 0.203125, 0.18181818181818182, 0.09448818897637795, 0.15483870967741936, 0.2247191011235955, 0.12389380530973451, 0.11290322580645161, 0.20689655172413796, 0.13157894736842102]]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our models summaries we can compare them to our Human made ones using Rouge\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "allScores = [[],[],[]]\n",
    "for i in range(len(modelSumms)):\n",
    "    score = scorer.score(target=humanSumm[i],prediction=modelSumms[i])\n",
    "    r1fscore = score['rouge1'].fmeasure\n",
    "    r2fscore = score['rouge2'].fmeasure\n",
    "    rLfscore = score['rougeL'].fmeasure\n",
    "    allScores[0].append(r1fscore)\n",
    "    allScores[1].append(r2fscore)\n",
    "    allScores[2].append(rLfscore)\n",
    "\n",
    "# List of F-scores in the order ['rouge1', 'rouge2', 'rougeL']\n",
    "print(allScores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Ranks rouge-1 f-score = \n",
      "0.30111254956762207\n",
      "Text Ranks rouge-2 f-score = \n",
      "0.07447425320840831\n",
      "Text Ranks rouge-L f-score = \n",
      "0.16958473115460615\n"
     ]
    }
   ],
   "source": [
    "# Getting the average F-score of the three metrics\n",
    "\n",
    "alg1rouge1 = sum(allScores[0]) / len(allScores[0])\n",
    "alg1rouge2 = sum(allScores[1]) / len(allScores[1])\n",
    "alg1rougeL = sum(allScores[2]) / len(allScores[2])\n",
    "\n",
    "print(\"Text Ranks rouge-1 f-score = \")\n",
    "print(alg1rouge1)\n",
    "print(\"Text Ranks rouge-2 f-score = \")\n",
    "print(alg1rouge2)\n",
    "print(\"Text Ranks rouge-L f-score = \")\n",
    "print(alg1rougeL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# LSI (Latent Sentiment Indexing): Multi-document summarization\n",
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to multiple text files\n",
    "    Output: List of multiple text\n",
    "'''\n",
    "def txt_files_to_string(filepaths) -> list[list[str]]:\n",
    "    i = 0\n",
    "    document_list = []\n",
    "    for file in filepaths:\n",
    "        with open(file, 'r', encoding='utf8') as file:\n",
    "            data = file.read()\n",
    "            data = data.replace('\\n', ' ') # Remove newline characters\n",
    "            document_list.append(data)\n",
    "    return document_list\n",
    "#print(data)\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "document_filepath_1 = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_filepath_2 = 'Documents/Japanese_Earthquake-Britannica.txt'\n",
    "documents = [document_filepath_1, document_filepath_2]\n",
    "document_text_list = txt_files_to_string(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Tokenize, remove stopwords, and stem the words\n",
    "def preprocess_lsi_text(document_list) -> list:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_docs = []\n",
    "    tokenized_documents = []\n",
    "    \n",
    "    for doc in document_list:\n",
    "    # Tokenizer\n",
    "        tokenized_sentences = sent_tokenize(doc, language='english')\n",
    "        tokenized_documents.append(tokenized_sentences)\n",
    "    # LowerCase\n",
    "        sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "    # Remove Punctuation\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "    # Remove Stop words\n",
    "        removed_stop_words = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_no_punctuation]\n",
    "    \n",
    "    # Stemming\n",
    "        stemmed_words = []\n",
    "        stemmed_sentences = []\n",
    "        for sentences in removed_stop_words:\n",
    "            stemmed_words = []\n",
    "            for word in sentences:\n",
    "                stemmed_words.append(stemmer.stem(word))\n",
    "            stemmed_sentences.append(stemmed_words)\n",
    "            \n",
    "        processed_docs.append(stemmed_sentences)\n",
    "    return processed_docs, tokenized_documents\n",
    "\n",
    "processed_docs, tokenized_documents = (preprocess_lsi_text(document_text_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Sentiment Indexing): Term Frequency\n",
    "# Create a dictionary mapping\n",
    "dictionary = corpora.Dictionary(processed_docs[1])\n",
    "# Bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs[1]]\n",
    "def create_dict_bow(documents):\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs[1]]\n",
    "    return dictionary, bow_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.355*\"wave\" + 0.293*\"coast\" + 0.228*\"island\" + 0.223*\"mile\"'), (1, '0.395*\"km\" + 0.395*\"mile\" + -0.193*\"coast\" + -0.176*\"wave\"')]\n"
     ]
    }
   ],
   "source": [
    "# LSI (Latent Sentiment Indexing): Create LSI Model using Gensim\n",
    "lsi_model = LsiModel(bow_corpus, num_topics=2, id2word=dictionary)\n",
    "lsi_text = [lsi_model[doc] for doc in bow_corpus]\n",
    "doc_rep = lsi_model[bow_corpus]\n",
    "print(lsi_model.print_topics(num_topics=2, num_words=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 3.6111562305864964), (8, 4.243492721316155)]\n",
      "[(8, 4.243492721316155), (25, 3.6111562305864964)]\n"
     ]
    }
   ],
   "source": [
    "def takenext(elem):\n",
    "    return elem[1]\n",
    "\n",
    "vecsSort = list(map(lambda i: list(), range(2)))\n",
    "for i, docv in enumerate(doc_rep):\n",
    "    for sc in docv:\n",
    "        isent = (i, abs(sc[1]))\n",
    "        vecsSort[sc[0]].append(isent)\n",
    "vecsSort = list(map(lambda x: sorted(x, key=takenext, reverse=True), vecsSort))\n",
    "\n",
    "def select_top_sent_lsi(summSize, numTopics, sortedVec):\n",
    "    topSentences = []\n",
    "    sent_no = []\n",
    "    sentInd = set()\n",
    "    sCount = 0\n",
    "    for i in range(summSize):\n",
    "        for j in range(numTopics):\n",
    "            vecs = sortedVec[j]\n",
    "            si = vecs[i][0]\n",
    "            if si not in sentInd:\n",
    "                sent_no.append(vecs[i])\n",
    "                topSentences.append(vecs[i])\n",
    "                sentInd.add(si)\n",
    "                sCount += 1\n",
    "                if sCount == summSize:\n",
    "                    return sent_no\n",
    "top = select_top_sent_lsi(2, 2, vecsSort)\n",
    "print(top)\n",
    "top.sort()\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_list(top_sentences):\n",
    "    top_sentence_list = []\n",
    "    for i in top_sentences:\n",
    "        top_sentence_list.append(i[0])\n",
    "    return top_sentence_list\n",
    "\n",
    "top_sentence_list = create_sentence_list(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On March 11, 2011, Japan experienced the strongest earthquake in its recorded history.', 'The earthquake struck below the North Pacific, 130 kilometers (81 miles) east of Sendai, the largest city in the Tohoku region, a northern part of the island of Honshu.', 'The Tohoku earthquake caused a tsunami.', 'A tsunami—Japanese for “harbor wave”—is a series of powerful waves caused by the displacement of a large body of water.', 'Most tsunamis, like the one that formed off Tohoku, are triggered by underwater tectonic activity, such as earthquakes and volcanic eruptions.', 'The Tohoku tsunami produced waves up to 40 meters (132 feet) high, More than 450,000 people became homeless as a result of the tsunami.', 'More than 15,500 people died.', 'The tsunami also severely crippled the infrastructure of the country.', 'In addition to the thousands of destroyed homes, businesses, roads, and railways, the tsunami caused the meltdown of three nuclear reactors at the Fukushima Daiichi Nuclear Power Plant.', 'The Fukushima nuclear disaster released toxic, radioactive materials into the environment and forced thousands of people to evacuate their homes and businesses.']\n",
      "['Japan earthquake and tsunami of 2011, severe natural disaster that occurred in northeastern Japan on March 11, 2011.', 'The event began with a powerful earthquake off the northeastern coast of Honshu, Japan’s main island, which caused widespread damage on land and initiated a series of large tsunami waves that devastated many coastal areas of the country, most notably in the Tōhoku region (northeastern Honshu).', 'The tsunami also instigated a major nuclear accident at a power station along the coast.', 'The magnitude-9.0 earthquake struck at 2:46 PM.', '(The early estimate of magnitude 8.9 was later revised upward.)', 'The epicentre was located some 80 miles (130 km) east of the city of Sendai, Miyagi prefecture, and the focus occurred at a depth of 18.6 miles (about 30 km) below the floor of the western Pacific Ocean.', 'The earthquake was caused by the rupture of a stretch of the subduction zone associated with the Japan Trench, which separates the Eurasian Plate from the subducting Pacific Plate.', '(Some geologists argue that this portion of the Eurasian Plate is actually a fragment of the North American Plate called the Okhotsk microplate.)', 'A part of the subduction zone measuring approximately 190 miles (300 km) long by 95 miles (150 km) wide lurched as much as 164 feet (50 metres) to the east-southeast and thrust upward about 33 feet (10 metres).', 'The March 11 temblor was felt as far away as Petropavlovsk-Kamchatsky, Russia; Kao-hsiung, Taiwan; and Beijing, China.', 'It was preceded by several foreshocks, including a magnitude-7.2 event centred approximately 25 miles (40 km) away from the epicentre of the main quake.', 'Hundreds of aftershocks, dozens of magnitude 6.0 or greater and two of magnitude 7.0 or greater, followed in the days and weeks after the main quake.', '(Nearly two years later, on December 7, 2012, a magnitude-7.3 tremor originated from the same plate boundary region.', 'The quake caused no injuries and little damage.)', 'The March 11, 2011, earthquake was the strongest to strike the region since the beginning of record keeping in the late 19th century, and it is considered one of the most powerful earthquakes ever recorded.', 'It was later reported that a satellite orbiting at the outer edge of Earth’s atmosphere that day had detected infrasonics (very low-frequency sound waves) from the quake.', 'The sudden horizontal and vertical thrusting of the Pacific Plate, which has been slowly advancing under the Eurasian Plate near Japan, displaced the water above and spawned a series of highly destructive tsunami waves.', 'A wave measuring some 33 feet high inundated the coast and flooded parts of the city of Sendai, including its airport and the surrounding countryside.', 'According to some reports, one wave penetrated some 6 miles (10 km) inland after causing the Natori River, which separates Sendai from the city of Natori to the south, to overflow.', 'Damaging tsunami waves struck the coasts of Iwate prefecture, just north of Miyagi prefecture, and Fukushima, Ibaraki, and Chiba, the prefectures extending along the Pacific coast south of Miyagi.', 'In addition to Sendai, other communities hard-hit by the tsunami included Kamaishi and Miyako in Iwate; Ishinomaki, Kesennuma, and Shiogama in Miyagi; and Kitaibaraki and Hitachinaka in Ibaraki.', 'As the floodwaters retreated back to the sea, they carried with them enormous quantities of debris, as well as thousands of victims caught in the deluge.', 'Large stretches of land were left submerged under seawater, particularly in lower-lying areas.', 'The earthquake triggered tsunami warnings throughout the Pacific basin.', 'The tsunami raced outward from the epicentre at speeds that approached about 500 miles (800 km) per hour.', 'It generated waves 11 to 12 feet (3.3 to 3.6 metres) high along the coasts of Kauai and Hawaii in the Hawaiian Islands chain and 5-foot (1.5-metre) waves along the island of Shemya in the Aleutian Islands chain.', 'Several hours later 9-foot (2.7-metre) tsunami waves struck the coasts of California and Oregon in North America.', 'Finally, some 18 hours after the quake, waves roughly 1 foot (0.3 metre) high reached the coast of Antarctica and caused a portion of the Sulzberger Ice Shelf to break off its outer edge.']\n",
      "\n",
      "\n",
      "A part of the subduction zone measuring approximately 190 miles (300 km) long by 95 miles (150 km) wide lurched as much as 164 feet (50 metres) to the east-southeast and thrust upward about 33 feet (10 metres). It generated waves 11 to 12 feet (3.3 to 3.6 metres) high along the coasts of Kauai and Hawaii in the Hawaiian Islands chain and 5-foot (1.5-metre) waves along the island of Shemya in the Aleutian Islands chain.\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "doc = []\n",
    "count = 0\n",
    "#print(tokenized_documents)\n",
    "for document in tokenized_documents:\n",
    "    print(document)\n",
    "    count = 0\n",
    "    summary = []\n",
    "    for sentence in document:\n",
    "        doc.append(sentence)\n",
    "        if count in top_sentence_list:\n",
    "            summary.append(sentence)\n",
    "        count += 1\n",
    "\n",
    "    \n",
    "summary = \" \".join(summary)\n",
    "doc = \" \".join(doc)\n",
    "print()\n",
    "#print(doc)\n",
    "print()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "##### The following tutorials helped us implement the algorithms in the document:\n",
    "##### 1. https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n",
    "##### 2. https://towardsdatascience.com/document-summarization-using-latent-semantic-indexing-b747ef2d2af6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
