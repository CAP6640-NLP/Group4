{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP 6640 \n",
    "### Project 1 - Extractive Summarization\n",
    "### Feb 8, 2024\n",
    "\n",
    "### Group 4\n",
    "### Andres Graterol\n",
    "###                   UCF ID: 4031393\n",
    "### Zachary Lyons\n",
    "###                   UCF ID: 4226832\n",
    "### Christopher Hinkle\n",
    "###                   UCF ID: 4038573\n",
    "### Nicolas Leocadio\n",
    "###                   UCF ID: 3791733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmp\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tmp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tmp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, LsiModel\n",
    "from gensim import corpora\n",
    "from scipy import spatial\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to a text file\n",
    "    Output: String of the text file\n",
    "'''\n",
    "def txt_file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        data = file.read()\n",
    "        data = data.replace('\\n', ' ') # Remove newline characters\n",
    "    return data\n",
    "\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_text = txt_file_to_string(document_filepath)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['march', '11', '2011', 'japan', 'experienced', 'strongest', 'earthquake', 'recorded', 'history'], ['earthquake', 'struck', 'north', 'pacific', '130', 'kilometers', '81', 'miles', 'east', 'sendai', 'largest', 'city', 'tohoku', 'region', 'northern', 'part', 'island', 'honshu'], ['tohoku', 'earthquake', 'caused', 'tsunami'], ['tsunami—japanese', '“harbor', 'wave”—is', 'series', 'powerful', 'waves', 'caused', 'displacement', 'large', 'body', 'water'], ['tsunamis', 'like', 'one', 'formed', 'tohoku', 'triggered', 'underwater', 'tectonic', 'activity', 'earthquakes', 'volcanic', 'eruptions'], ['tohoku', 'tsunami', 'produced', 'waves', '40', 'meters', '132', 'feet', 'high', '450000', 'people', 'became', 'homeless', 'result', 'tsunami'], ['15500', 'people', 'died'], ['tsunami', 'also', 'severely', 'crippled', 'infrastructure', 'country'], ['addition', 'thousands', 'destroyed', 'homes', 'businesses', 'roads', 'railways', 'tsunami', 'caused', 'meltdown', 'three', 'nuclear', 'reactors', 'fukushima', 'daiichi', 'nuclear', 'power', 'plant'], ['fukushima', 'nuclear', 'disaster', 'released', 'toxic', 'radioactive', 'materials', 'environment', 'forced', 'thousands', 'people', 'evacuate', 'homes', 'businesses']]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: remove punctuation, tokenize, and remove stopwords\n",
    "\n",
    "'''\n",
    "    Purpose: Perform appropriate preprocessing on the text file for the TextRank algorithm\n",
    "'''\n",
    "def preprocess_text(text, stop_words):\n",
    "    tokenized_sentences = sent_tokenize(text, language='english')\n",
    "\n",
    "    sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "\n",
    "    # Regular Expression to match any punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Remove the punctuation from the lowercase sentences\n",
    "    sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "\n",
    "    data = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_no_punctuation]\n",
    "    return data, tokenized_sentences\n",
    "\n",
    "# Obtain stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Preprocess the text to obtain the data we will use going forward\n",
    "data, tokenized_sentences = preprocess_text(document_text, stop_words)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.70977305 0.6788758  0.90541902 0.85618414 0.77903019\n",
      "  0.58480879 0.82346425 0.70122287 0.81078446]\n",
      " [0.70977305 0.99999999 0.46256435 0.78552505 0.81960516 0.91196754\n",
      "  0.39324753 0.57507588 0.99913895 0.88168264]\n",
      " [0.6788758  0.46256435 0.99999998 0.61504091 0.56159318 0.52088329\n",
      "  0.8675951  0.81588685 0.47054226 0.54896826]\n",
      " [0.90541902 0.78552505 0.61504091 1.         0.95161855 0.85485302\n",
      "  0.52866716 0.74685248 0.77230259 0.89089428]\n",
      " [0.85618414 0.81960516 0.56159318 0.95161855 0.99999998 0.89274893\n",
      "  0.48374692 0.69104607 0.81084365 0.92575996]\n",
      " [0.77903019 0.91196754 0.52088329 0.85485302 0.89274893 0.99999998\n",
      "  0.44024469 0.63624942 0.90966395 0.96622476]\n",
      " [0.58480879 0.39324753 0.8675951  0.52866716 0.48374692 0.44024469\n",
      "  0.99999999 0.69989115 0.40420296 0.47373378]\n",
      " [0.82346425 0.57507588 0.81588685 0.74685248 0.69104607 0.63624942\n",
      "  0.69989115 0.99999999 0.57367478 0.66818031]\n",
      " [0.70122287 0.99913895 0.47054226 0.77230259 0.81084365 0.90966395\n",
      "  0.40420296 0.57367478 0.99999999 0.87944682]\n",
      " [0.81078446 0.88168264 0.54896826 0.89089428 0.92575996 0.96622476\n",
      "  0.47373378 0.66818031 0.87944682 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TextRank: Word Embeddings \n",
    " \n",
    "# Grab the maximum number of words in a sentence for padding sentence embeddings\n",
    "max_sentence_length = max([len(sentence) for sentence in data])\n",
    "\n",
    "'''\n",
    "    Train the Word2Vec model on the data and calculate embeddings for each word\n",
    "        min_count: Ignores all words with total frequency lower than this\n",
    "        vector_size: Dimensionality of the word vectors\n",
    "'''\n",
    "# NOTE: If output is unsatsifactory, train for longer epochs\n",
    "model = Word2Vec(data, min_count=1, vector_size=1, epochs=5000)\n",
    "\n",
    "# Grab sentence embeddings by leveraging the word embeddings and sentence tokens\n",
    "sentence_embeddings = [[model.wv[word][0] for word in words] for words in data]\n",
    "\n",
    "# Pad the sentence embeddings with 0's to ensure all sentences have the same length\n",
    "sentence_embeddings = [np.pad(embedding, (0, max_sentence_length - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "# Calculate the similarity matrix\n",
    "# Instantiate a matrix of zeros with the same shape as the number of sentences\n",
    "similarity_matrix = np.zeros([len(data), len(data)])\n",
    "\n",
    "# Populate the similarity matrix with cosine similarity scores (same as 1 - cosine distance)\n",
    "for i, row in enumerate(sentence_embeddings):\n",
    "    for j, col in enumerate(sentence_embeddings):\n",
    "        similarity_matrix[i][j] = 1 - spatial.distance.cosine(row, col)\n",
    "\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank: Call nx's pagerank to get scores. \n",
    "\n",
    "''' \n",
    "    Get the top n sentences from pagerank scores\n",
    "'''\n",
    "def top_n_sentences(n, scores, tokenized_sentences):\n",
    "    # Key => Sentence \n",
    "    # Value => PageRank Score\n",
    "    sentence_score_dict = {sentence:scores[i] for i, sentence in enumerate(tokenized_sentences)}\n",
    "\n",
    "    # Filter the dictionary to contain only the top n sentences\n",
    "    top_sentences = dict(sorted(sentence_score_dict.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "# Convert similarity matrix to an nx graph and call nx's pagerank\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(graph)\n",
    "\n",
    "# NOTE: Modify this variable to change the number of sentences in the summary\n",
    "num_sent_to_extract = 4\n",
    "\n",
    "extractive_summary = top_n_sentences(num_sent_to_extract, scores, tokenized_sentences)\n",
    "\n",
    "# Iterate through the dictionary to output the summary\n",
    "for sentence, score in extractive_summary.items():\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluation will depend on the method used to implement extractive summarization\n",
    "#       - ILP (Integer Linear Programming): We can use ROUGE-2 for evaluation\n",
    "# Andres NOTE: This is the only section that I am unsure of. It would be cool to use ROUGE-2 to compare our TextRank algorithm to the bigram inspection\n",
    "\n",
    "\n",
    "def csv_column_to_list(file_path, column_index):\n",
    "    column_data = []\n",
    "    with open(file_path, encoding=\"utf8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if len(row) > column_index:  # Ensure the row has the desired column\n",
    "                column_data.append(row[column_index].replace(\"\\n\",\" \"))\n",
    "\n",
    "    return column_data\n",
    "\n",
    "csvFile = \"./Dataset/CnnTestData.csv\"\n",
    "\n",
    "# Get the list of articles and human summaries that we are going to be evaluating\n",
    "testDocs = csv_column_to_list(csvFile,1)\n",
    "testDocs = testDocs[1:21]\n",
    "\n",
    "humanSumm = csv_column_to_list(csvFile,2)\n",
    "humanSumm = humanSumm[1:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan's Misbah-ul-Haq (left) and Wahab Riaz look set to play international cricket in their homeland again .Security officials display arms and ammunition seized after the terrorists' attack on the Sri Lanka team .Sri Lankan cricketers were rescued by the Pakistani air force from Gadaffi Stadium after the terror attack .Pakistan appear set to host Test-playing opposition in their home country for the first time in more than six years.\n"
     ]
    }
   ],
   "source": [
    "# Get our models summarizations of the documents\n",
    "modelSumms = []\n",
    "\n",
    "for doc in testDocs:\n",
    "    data, tokenized_sentences = preprocess_text(doc, stop_words)\n",
    "    max_sentence_length = max([len(sentence) for sentence in data])\n",
    "    model = Word2Vec(data, min_count=1, vector_size=1, epochs=5000)\n",
    "\n",
    "    # Grab sentence embeddings by leveraging the word embeddings and sentence tokens\n",
    "    sentence_embeddings = [[model.wv[word][0] for word in words] for words in data]\n",
    "\n",
    "    # Pad the sentence embeddings with 0's to ensure all sentences have the same length\n",
    "    sentence_embeddings = [np.pad(embedding, (0, max_sentence_length - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "    # Calculate the similarity matrix\n",
    "    # Instantiate a matrix of zeros with the same shape as the number of sentences\n",
    "    similarity_matrix = np.zeros([len(data), len(data)])\n",
    "\n",
    "    # Populate the similarity matrix with cosine similarity scores (same as 1 - cosine distance)\n",
    "    for i, row in enumerate(sentence_embeddings):\n",
    "        for j, col in enumerate(sentence_embeddings):\n",
    "            similarity_matrix[i][j] = 1 - spatial.distance.cosine(row, col)\n",
    "\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(graph)\n",
    "    # NOTE: Modify this variable to change the number of sentences in the summary\n",
    "    num_sent_to_extract = 4\n",
    "\n",
    "    extractive_summary = top_n_sentences(num_sent_to_extract, scores, tokenized_sentences)\n",
    "\n",
    "    # Iterate through the dictionary to output the summary\n",
    "    s = \"\"\n",
    "    for sentence, score in extractive_summary.items():\n",
    "        s = s + sentence\n",
    "    \n",
    "    modelSumms.append(s)\n",
    "\n",
    "print(modelSumms[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3114754098360656, 0.03333333333333333, 0.1639344262295082], [0.25, 0.07936507936507936, 0.18750000000000003], [0.34532374100719426, 0.10218978102189781, 0.2158273381294964], [0.1806451612903226, 0.0261437908496732, 0.0903225806451613], [0.26277372262773724, 0.02962962962962963, 0.13138686131386862], [0.34328358208955223, 0.10606060606060606, 0.23880597014925375], [0.3793103448275862, 0.2280701754385965, 0.2586206896551724], [0.3783783783783784, 0.09589041095890412, 0.22972972972972974], [0.2096774193548387, 0.06557377049180328, 0.12903225806451613], [0.271604938271605, 0.05, 0.1358024691358025], [0.411764705882353, 0.04477611940298507, 0.17647058823529413], [0.359375, 0.07936507936507937, 0.203125], [0.2987012987012987, 0.09210526315789473, 0.18181818181818182], [0.1889763779527559, 0.015999999999999997, 0.09448818897637795], [0.2967741935483871, 0.05228758169934641, 0.15483870967741936], [0.3595505617977528, 0.11363636363636365, 0.2247191011235955], [0.24778761061946902, 0.036036036036036036, 0.12389380530973451], [0.2580645161290323, 0.09836065573770492, 0.11290322580645161], [0.3793103448275862, 0.10526315789473684, 0.20689655172413796], [0.2894736842105263, 0.03539823008849558, 0.13157894736842102]]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our models summaries we can compare them to our Human made ones using Rouge\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "allScores = []\n",
    "for i in range(len(modelSumms)):\n",
    "    score = scorer.score(target=humanSumm[i],prediction=modelSumms[i])\n",
    "    r1fscore = score['rouge1'].fmeasure\n",
    "    r2fscore = score['rouge2'].fmeasure\n",
    "    rLfscore = score['rougeL'].fmeasure\n",
    "    allScores.append([r1fscore,r2fscore,rLfscore])\n",
    "\n",
    "# List of F-scores in the order ['rouge1', 'rouge2', 'rougeL']\n",
    "print(allScores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan's Misbah-ul-Haq (left) and Wahab Riaz look set to play international cricket in their homeland again .Security officials display arms and ammunition seized after the terrorists' attack on the Sri Lanka team .Sri Lankan cricketers were rescued by the Pakistani air force from Gadaffi Stadium after the terror attack .Pakistan appear set to host Test-playing opposition in their home country for the first time in more than six years.\n",
      "Zimbabwe have reportedly agreed to visit Pakistan for ODI series in May .Pakistan have not played host to major cricket series since 2009 .There have been security fears since Sri Lanka were victims of terror attack .Team bus was targeted by gunmen in Lahore, and eight people were killed .\n",
      "Pakistan appear set to host Test-playing opposition in their home country for the first time in more than six years. Zimbabwe have reportedly agreed to travel for a short one-day international series next month, likely to take place in Lahore and Karachi. No tourists have played in Pakistan because of security fears since the terrorist attack on Sri Lanka's team bus in Lahore in March 2009 - when six policemen and two civilians died, and several of the tourists were injured. A policeman weeps over the covered body of one of his colleagues killed when gunmen attacked the Sri Lankan cricket team in March 2009. Their team bus came under fire while travelling through the city of Lahore . Sri Lankan cricketers were rescued by the Pakistani air force from Gadaffi Stadium after the terror attack . Security officials display arms and ammunition seized after the terrorists' attack on the Sri Lanka team . 'Home' Test and ODI series' in the meantime have taken place in the United Arab Emirates, England's destination for the second time this autumn, a tour for which an exact schedule is still to be confirmed. The details of Zimbabwe's proposed trip are also not yet known. But Pakistan Cricket Board president Shaharyar Khan has stated that, after discussions with his Zimbabwean opposite number Wilson Manase, he expects the short tour to go ahead. Pakistan's Misbah-ul-Haq (left) and Wahab Riaz look set to play international cricket in their homeland again . Shaharyar said: 'He (Manase) told me they are coming and they are coming with their full team - but (just) for a week. 'They will send a security team to assess the situation, following which they will confirm the tour. So they will come in the middle of May, and our efforts will be to host matches in Lahore and Karachi.'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# LSI (Latent Sentiment Indexing): Multi-document summarization\n",
    "# Gather lengthy articles or a collection of documents that all relate to the same topic (i.e. documents covering an earthquake)\n",
    "# TextRank: Single-document summarization\n",
    "\n",
    "'''\n",
    "    Input: File path to multiple text files\n",
    "    Output: List of multiple text\n",
    "'''\n",
    "def txt_files_to_string(filepaths) -> list[list[str]]:\n",
    "    i = 0\n",
    "    document_list = []\n",
    "    for file in filepaths:\n",
    "        with open(file, 'r', encoding='utf8') as file:\n",
    "            data = file.read()\n",
    "            data = data.replace('\\n', ' ') # Remove newline characters\n",
    "            document_list.append(data)\n",
    "    return document_list\n",
    "#print(data)\n",
    "# Data is located in text format, character escaped, inside the Documents folder\n",
    "# TODO: This is a very short sample document to test functionality. When we confirm this works, lets use a larger document.\n",
    "document_filepath_1 = 'Documents/Japanese_Earthquake-NationalGeographic.txt'\n",
    "document_filepath_2 = 'Documents/Japanese_Earthquake-Britannica.txt'\n",
    "documents = [document_filepath_1, document_filepath_2]\n",
    "document_text_list = txt_files_to_string(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<bound method Dictionary.doc2bow of <gensim.corpora.dictionary.Dictionary object at 0x000001755E4BBD50>>, <bound method Dictionary.doc2bow of <gensim.corpora.dictionary.Dictionary object at 0x000001755E4BBD50>>]\n"
     ]
    }
   ],
   "source": [
    "# LSI (Latent Sentiment Indexing): Tokenize, remove stopwords, and stem the words\n",
    "def preprocess_lsi_text(document_list) -> list:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_docs = []\n",
    "    for doc in document_list:\n",
    "    # Tokenizer\n",
    "        tokenized_sentences = sent_tokenize(doc, language='english')  \n",
    "    # LowerCase\n",
    "        sentences_to_lower = [sentence.lower() for sentence in tokenized_sentences]\n",
    "    # Remove Punctuation\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        sentences_no_punctuation = [regex.sub('', sentence) for sentence in sentences_to_lower]\n",
    "    # Remove Stop words\n",
    "        removed_stop_words = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_no_punctuation]\n",
    "    # Stemming\n",
    "        stemmed_words = []\n",
    "        for sentences in removed_stop_words:\n",
    "            for word in sentences:\n",
    "                stemmed_words.append(stemmer.stem(word))\n",
    "        \n",
    "        processed_docs.append(stemmed_words)\n",
    "    return processed_docs\n",
    "    # Build Dictionary\n",
    "    # Bag of wor\n",
    "processed_docs = preprocess_lsi_text(document_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel<num_terms=317, num_topics=2, decay=1.0, chunksize=20000>\n"
     ]
    }
   ],
   "source": [
    "# LSI (Latent Sentiment Indexing): Term Frequency\n",
    "# Create a dictionary mapping\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "# Bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Algorithm and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (0,) not aligned: 2 (dim 0) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Generate and print summaries for each document\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary_for_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(summaries):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary for Document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[67], line 30\u001b[0m, in \u001b[0;36mgenerate_summary_for_documents\u001b[1;34m(documents, num_sentences)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m     29\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(doc)\n\u001b[1;32m---> 30\u001b[0m     ranked_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mrank_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlsi_text\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     summary_sentences \u001b[38;5;241m=\u001b[39m [sentence[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m ranked_sentences[:num_sentences]]\n\u001b[0;32m     32\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(summary_sentences)\n",
      "Cell \u001b[1;32mIn[67], line 20\u001b[0m, in \u001b[0;36mrank_sentences\u001b[1;34m(sentences, lsi_doc)\u001b[0m\n\u001b[0;32m     18\u001b[0m     sentence_bow \u001b[38;5;241m=\u001b[39m dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(sentence_tokens)\n\u001b[0;32m     19\u001b[0m     sentence_lsi \u001b[38;5;241m=\u001b[39m lsi_model[sentence_bow]\n\u001b[1;32m---> 20\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlsi_doc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence_lsi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     ranked_sentences\u001b[38;5;241m.\u001b[39mappend((sentence, similarity))\n\u001b[0;32m     22\u001b[0m ranked_sentences\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (0,) not aligned: 2 (dim 0) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "# LSI (Latent Sentiment Indexing): Create LSI Model using Gensim\n",
    "lsi_model = LsiModel(bow_corpus,num_topics=2, id2word= dictionary)\n",
    "# Sort documents by weight \n",
    "lsi_text = [lsi_model[doc] for doc in bow_corpus]\n",
    "\n",
    "# Check if lsi_text is empty before accessing its elements\n",
    "    # Sort vectors by score \n",
    "\n",
    "    # Select top documents \n",
    "\n",
    "    # Sort sentence numbers in order \n",
    "\n",
    "    # Obtain the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "##### The following tutorials helped us implement the algorithms in the document:\n",
    "##### 1. https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n",
    "##### 2. https://towardsdatascience.com/document-summarization-using-latent-semantic-indexing-b747ef2d2af6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
